{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist, cifar10\n",
    "from keras.layers import Dense, Flatten, Normalization, Dropout, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomZoom, BatchNormalization, Activation, AveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Images\n",
    "\n",
    "Images are perhaps the place where neural networks have had the most dramatic impact. The best neural networks can very accurately perform image recognition, to the point that they can identify disease in medical imaging better than doctors, or track people's faces (or even their gait) in real time video. \n",
    "\n",
    "Image processing and applications such as image recognition is one of the most visible and exciting applications of neural networks. In contrast to our previous models that can perform image classification, the structure of neural networks allows them to be tailored to be very good at dealing with image data. This also tends to be true for many \"analog\" data types, such as audio, video, and text, which is why many of the most impressive current applications of AI, like live translation and facial recognition, use neural networks. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神經網絡和圖像\n",
    "\n",
    "圖像可能是神經網絡產生最顯著影響的地方。 \n",
    "\n",
    "最好的神經網絡可以非常準確地執行圖像識別，以至於它們可以比醫生更好地識別醫學影像中的疾病，\n",
    "\n",
    "或者在實時視頻中跟踪人臉（甚至他們的步態）。\n",
    "\n",
    "圖像處理和圖像識別等應用是神經網絡最引人注目和令人興奮的應用之一。 \n",
    "\n",
    "與我們之前可以執行圖像分類的模型相比，神經網絡的結構允許它們被定制以非常擅長處理圖像數據。 \n",
    "\n",
    "這也適用於許多“模擬”數據類型，例如音頻、視頻和文本，\n",
    "\n",
    "這就是為什麼當前許多最令人印象深刻的 AI 應用程序（例如實時翻譯和麵部識別）都使用神經網絡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data\n",
    "\n",
    "Until now we've used simple images that only have one color, we can expand this a bit now to handle more \"normal\" images. We will use one of the sample ones from Keras called cifar10. \n",
    "\n",
    "### Color Images\n",
    "\n",
    "Color images have a greater depth - one layer for each color. Usually this is one for red, blue, and green, or RGB. There are other color encodings, but the idea is pretty similar. Of note for us, these images are now 3 dimensional - in terms of their representation as an array. \n",
    "\n",
    "![RGB](images/rgb.png \"RGB\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization in Images\n",
    "\n",
    "One common difference in dealing with image data is the normalization process. Images, at least with the RGB encoding we are looking at, have their values encoded on a standardized scale, normally 0 to 255. Because of this, rescaling the data to 0 and 1 can be as simple as dividing by 255. Since every feature (pixel) is on the same scale, this works exactly the same as using a MinMaxScaler."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 圖像規範化\n",
    "\n",
    "處理圖像數據的一個常見差異是規範化過程。 \n",
    "\n",
    "圖像，至少使用我們正在查看的 RGB 編碼，將它們的值按標準化比例編碼，通常為 0 到 255。\n",
    "\n",
    "因此，將數據重新縮放為 0 和 1 可以像除以 255 一樣簡單。\n",
    "\n",
    "由於每個特徵 （像素）在相同的比例上，這與使用 MinMaxScaler 完全相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Note: the class names are taken from the documentation\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[7].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Shapes and Color\n",
    "\n",
    "One of our images, number 7 in the dataset, has the dimensions 32x32x3. This means that it is 32 pixels wide, 32 pixels tall, and has 3 layers of color - red, green, and blue. Each image is effectively the 3 color layers stacked on top of each other to generate a full color image.\n",
    "\n",
    "There are other ways to encode image data, or different color spaces. RGB is probably the most common, but other ones exist and may perform better in certain scenarios. The concept is the same, but the color data is broken up into different components other than red, green, and blue. For example, a lot of image processing is done in YCbCr, which is a color space that tends to be more efficient for image processing. We won't go into the details of different color encodings, if we need to deal with images that are defined in that color space, we can use a library to convert them to RGB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 圖像形狀和顏色\n",
    "\n",
    "我們的一張圖像，數據集中的編號 7，尺寸為 32x32x3。 \n",
    "\n",
    "這意味著它的寬度為 32 像素，高度為 32 像素，並且具有 3 層顏色 - 紅色、綠色和藍色。 \n",
    "\n",
    "每個圖像實際上是 3 個顏色層相互堆疊以生成全彩色圖像。\n",
    "\n",
    "還有其他方法可以對圖像數據或不同的顏色空間進行編碼。 \n",
    "\n",
    "RGB 可能是最常見的，但也存在其他顏色，並且在某些情況下可能表現更好。 \n",
    "\n",
    "概念是一樣的，但顏色數據被分解成紅、綠、藍以外的不同成分。 \n",
    "\n",
    "例如，很多圖像處理都是在 YCbCr 中完成的，YCbCr 是一種圖像處理效率更高的顏色空間。 \n",
    "\n",
    "我們不會深入討論不同顏色編碼的細節，如果我們需要處理在該顏色空間中定義的圖像，\n",
    "\n",
    "我們可以使用一個庫將它們轉換為 RGB。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing\n",
    "\n",
    "We can use imshow to display one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[7])\n",
    "plt.xlabel(class_names[y_train[7][0]]) #The CIFAR labels happen to be arrays, so we need the extra index\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "We can make a simple helper to display an image. We can also use our loss plotting function from before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(data, labels, names, index):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(data[index])\n",
    "    plt.xlabel(names[labels[index][0]]) #The CIFAR labels happen to be arrays, so we need the extra index\n",
    "    plt.show()\n",
    "\n",
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def plot_acc(history):\n",
    "  plt.plot(history.history['accuracy'], label='accuracy')\n",
    "  plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the helper function to display an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(X_train, y_train, class_names, 192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Shape and Model\n",
    "\n",
    "Our data starts as images that are 32 x 32 x 3 - 32 pixels by 32 pixels by color depth of 3 (RGB).\n",
    "\n",
    "#### Flatten\n",
    "\n",
    "One new addition we can utilize is the Flatten layer, which does exactly what is says.  The flatten layer does the same thing we did when reshaping digit images, it makes them into a flat array. We specify the shape of one example of our dataset as the input shape argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - Activation and Loss\n",
    "\n",
    "Since we are doing a classification with Keras now we need to make a few small changes to handle that. \n",
    "\n",
    "#### Activation\n",
    "\n",
    "The first change is the activation on the output layer. When doing regression we want raw predictions, so there's no activation. Here we want to classify so we need to add activation. Since we are classifying into multiple classes we can use softmax to do so. We also have to set the units to the number of classes that we are predicting, in this case 10. \n",
    "\n",
    "Recall from when we first looked at multiclass classifications, the result of softmax is that we get a breakdown of probabilities that each record belongs to each of the classes, totalling to 1. Each class is represented by an output neuron, and the largest one wins and gets the label. \n",
    "\n",
    "#### Loss\n",
    "\n",
    "We also want to use different loss metrics when doing classifications. Here we will use categorical cross entropy. \n",
    "\n",
    "We will need to use to_categorical here to make our current labels (e.g. [4]) into a one-hot categorical array (e.g. [0,0,0,0,1,0,0,0,0,0]). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類——激活和損失\n",
    "\n",
    "由於我們現在正在使用 Keras 進行分類，因此我們需要進行一些小的更改來處理這個問題。\n",
    "\n",
    "#### 激活\n",
    "\n",
    "第一個變化是輸出層的激活。 在進行回歸時，我們需要原始預測，因此沒有激活。 這裡我們要分類所以需要加上激活。 \n",
    "\n",
    "由於我們要分類為多個類別，因此我們可以使用 softmax 來做到這一點。我們還必須將單位設置為我們預測的類數，在本例中為 10。\n",
    "\n",
    "回想一下我們第一次看多類分類時，softmax 的結果是我們得到了每條記錄屬於每個類的概率的細分，總計為 1。每個類由一個輸出神經元表示，最大的一個獲勝 並獲得標籤。\n",
    "\n",
    "＃＃＃＃ 損失\n",
    "\n",
    "我們還想在分類時使用不同的損失指標。 這裡我們將使用分類交叉熵。\n",
    "\n",
    "我們需要在這裡使用 to_categorical 將我們當前的標籤（例如 [4]）變成一個單熱分類數組（例如 [0,0,0,0,1,0,0,0,0,0]）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab Check\n",
    "\n",
    "I've added an example here of checking if we are in Colab. For me, I'm going to increase the default epochs and shrink the defauly batch size if so. The idea here is that I can run it on a limited basis locally when developing, but then run it on Colab when I want to do a full run. If you're old and senile like me, it helps to keep you from forgetting! We could do something similar for lots of stuff, we could take a subsample of the data for developing then use it all for training, or print verbose results when developing and nothing when training, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Base Parameters\n",
    "# Increase processing demands if on Colab\n",
    "BASE_EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "BASE_PATIENCE = 5\n",
    "MIN_DELTA = .02\n",
    "MONITOR = \"val_accuracy\"\n",
    "MODE = \"max\"\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  BASE_EPOCHS = 75\n",
    "  BATCH_SIZE = 32\n",
    "  BASE_PATIENCE = 10\n",
    "\n",
    "acc = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "pre = keras.metrics.Precision(name=\"precision\")\n",
    "rec = keras.metrics.Recall(name=\"recall\")\n",
    "metric_list = [acc, pre, rec]\n",
    "\n",
    "callback = EarlyStopping(monitor=MONITOR, patience=BASE_PATIENCE, restore_best_weights=True, min_delta=MIN_DELTA, mode=MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_train = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy: About 50% in Most Experiments\n",
    "\n",
    "Our simple model likely didn't do all that well. We can likely do better in making predictions! If we were simpletons we'd probably look for ways to cut back that obvious overfitting. We are super sophisticated though, so we'll get all crazy and try a totally different approach..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs - 卷積神經網絡\n",
    "\n",
    "為了更好地處理圖像，我們可以使用不同類型的神經網絡設計——CNN 或卷積神經網絡。 \n",
    "\n",
    "簡而言之，CNN 能夠“按原樣”查看圖像，捕獲空間關係，而將圖像處理為扁平陣列則不能。 \n",
    "\n",
    "當使用 CNN 時，我們可以首先在網絡的初始層中以原始尺寸處理圖像，\n",
    "\n",
    "然後將其展平以通過一組更熟悉的層進行最終預測。\n",
    "\n",
    "CNN 一點一點地看圖像，看一個小方塊，然後滑過幾個像素，看另一個方塊，等等。 \n",
    "\n",
    "這具有能夠從圖像區域中提取特徵的效果 - \n",
    "\n",
    "例如，想想自行車的圖像，當圖像通過圖像時，CNN 將能夠識別座椅或把手的不同形狀層。 \n",
    "\n",
    "這提高了模型識別定義圖像形狀的數據模式的能力，無論該形狀在圖像中的哪個位置。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN結構\n",
    "\n",
    "CNN 有一些新類型的層：\n",
    "<ul>\n",
    "<li> 卷積層 - 卷積層一次查看圖像的一小幀。\n",
    "<li> 池化層——池化層降低了數據的維度。\n",
    "<li> 常規神經網絡 - 在完成捲積部分的工作後，我們可以將數據展平並將其傳遞到最後一層的常規全連接網絡。\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 更複雜的圖像\n",
    "\n",
    "卷積層最容易被認為是一個顯微鏡，它在圖像上滾動，一次查看圖像的一個小方塊。 \n",
    "\n",
    "該圖像是我發現的顯示卷積過程的最具說明性的動畫； \n",
    "\n",
    "請注意，這個顯示的步幅為 2，這就是過濾器似乎跳躍的原因，也是輸出大小如此小的原因。 \n",
    "\n",
    "我們通常會依靠池化層來減小大小，並使用步長 1 和相同的填充來保持我們的輸出與輸入大小相同。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kernel](images/cnn_kernel.gif \"Kernel\" )\n",
    "\n",
    "這種卷積運算將輸入的“特徵圖”轉化為“輸出圖”。 \n",
    "\n",
    "轉換後的結果是每一層都捕獲圖像中的一些特徵——邊緣、方向等……並將它們映射到較低層。\n",
    "\n",
    "### 卷積層中的過濾器（又名內核）\n",
    "\n",
    "我們在製作卷積層時提供的參數之一是過濾器的數量。 \n",
    "\n",
    "我們有一大堆過濾器，隨著訓練的進行，每個過濾器都學會從圖像數據中找到一些不同的特徵。 \n",
    "\n",
    "另一個參數是過濾器的大小。 \n",
    "\n",
    "一般來說，過濾器尺寸是小方塊，例如 3 x 3 或 5 x 5，但隨著圖像的分辨率變得越來越高，\n",
    "\n",
    "計算機變得越來越快，更大的過濾器變得越來越普遍，典型的尺寸可能會繼續增加到一定程度。\n",
    "\n",
    "內核中的實際值是在 CNN 訓練期間學習的，與大多數其他事物一樣，它們是在梯度下降過程中的整個反向傳播步驟中確定的。 \n",
    "\n",
    "在我們通常使用的上下文中，圖像分類，過濾器中的權重被學習以最小化損失。 \n",
    "\n",
    "換句話說，過濾器正在學習找到對模型最重要的特徵，以便做出最佳預測。 \n",
    "\n",
    "或者，如果我們用一種不那麼書呆子的方式來表達它，過濾器就會被訓練來識別圖像中允許對其進行分類的部分。\n",
    "\n",
    "這部分有點像魔術，模型學習如何區分不同的圖像，並學習如何製作過濾器來提取所需的關鍵內容。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs - Convolutional Neural Networks\n",
    "\n",
    "To deal with images a little bit better we can use a different kind of neural network design - a CNN, or convolutional neural network. In short, a CNN is able to look at an image \"as it is\" caputuring spatial relationships that processing an image as a flattened array do not. When using a CNN we can first process the image in its original dimensions in the initial layers of the network, then flatten it down to go through a more familiar set of layers for the final prediction. \n",
    "\n",
    "A CNN looks at an image bit by bit, looking at a small square, then sliding over a few pixels, looking at another square, and so on. This has the effect of being able to extract features from areas of an image - as an example, think of an image of a bike, a CNN would be able to identify the distinct shape of a seat or handle bars as the image passes through the layers. This improves the ability of the model to identify patterns of data that define shapes in images, no matter where in the image that shape is.\n",
    "\n",
    "### CNN Structure\n",
    "\n",
    "A CNN has some new types of layers:\n",
    "<ul>\n",
    "<li> Convolutional layer - the convolutional layer looks at a small frame of the image at a time.\n",
    "<li> Pooling layer - the pooling layer reduced the dimensionality of the data. \n",
    "<li> Regular neural network - after the convolutional parts to their work, we can flatten the data and pass it to a regular fully connected network at the final layers. \n",
    "</ul>\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "##### Simple Image\n",
    "\n",
    "![CNN](images/cnn.gif \"CNN\")\n",
    "\n",
    "##### More Complex Image\n",
    "\n",
    "The convolutional layer is easiest to think of as a microscope that scrolls over an image looking at one small square of it at a time. This image is the most illustrative annimation I found of showing the convolution process; note that this one shows a stride of 2, which is why the filter seems to jump and is why the size of the output is so much smaller. We will normally rely on the pooling layer to reduce size, and use a stride of 1 with same padding to keep our outputs the same size as the inputs.  \n",
    "\n",
    "![Kernel](images/cnn_kernel.gif \"Kernel\" )\n",
    "\n",
    "\n",
    "This convolution operation translates the input \"feature map\" into an \"output map\". After the transformation the result is that each layer captures some features in the image - edges, orientation, etc... and map those down to lower layers. \n",
    "\n",
    "### Filters (a.k.a kernel) in the Convolutional Layer\n",
    "\n",
    "One of the arguemnts we provide when making a convolutional layer is the number of filters. We have a whole bunch of filters, each learns to find some different characteristic from the image data as the training progreses. Another argument is the size of the filter. In general, filter sizes are small squares such as 3 by 3 or 5 by 5, but as images become massively higher in resolution and computers become faster, larger filters are becoming more widespread and the typical size will likely continue to increase to some degree. \n",
    "\n",
    "The actual values in the kernel are learned during training in a CNN, like most other things, they are determined throughout the backpropagation steps in the gradient descent process. In the context that we are typically using, image classification, the weights in the filters are learned to minimize the loss. In other words, the filters are learning to find the features that are most important to the model in order to make the best predictions. Or if we phrase it in a less nerdy way, the filters are trained to identify the parts of the image that allow it to be classified. This part is somewhat like magic, the model learns how to differentiate the different images, and it learns how to make filters that extract the key things needed to do so. \n",
    "\n",
    "### Padding\n",
    "\n",
    "Padding is a setting that determines if the dimensionality of the data is reduced in the convolutional layer or not. We have two choices:\n",
    "\n",
    "<ul>\n",
    "<li> Valid padding - dimensions are reduced. \n",
    "<li> Same padding - dimensions are maintained. \n",
    "</ul>\n",
    "\n",
    "This is probably most easily illustrated by looking at the image above. That image is showing same padding - those 0s around the border are inserted to ensure that the kernel can start at the edge and still capture the entire picture. If this were valid padding the kernel would start at the real edge, and those edge values would never make it to the middle of the image. The resulting values will then be of a smaller dimension than the original. Using same padding allows the model to better capture the information around the border, avoiding what is known as the border effect. \n",
    "\n",
    "In general we should expect fractionally better performance with padding enabled, at the cost of some processing time and memory. This effect isn't usually massive, as with most pictures the valuable stuff is in the middle - so the impact depends on the dataset. If you think of the minst images or something like a passport photo, the edges are usually pretty unimportant. If you think of a picture caputered by a self driving car, the edges may be the most important, to see things like curbs and kids jumping in front of the car.\n",
    "\n",
    "#### Strides\n",
    "\n",
    "The stride value is how many pixels the kernel window shifts each time it looks at a window. Strides of 1 move 1 pixel at a time, larger strides \"skip\" some pixels. Our stride will usually just be 1. \n",
    "\n",
    "### Pooling Layer\n",
    "\n",
    "The pooling layer reduces the dimensionality of the data down. In image terms that you may have heard elsewhere we are downsampling - taking something that is at some higher resolution and transforming it to a lower resolution. \n",
    "\n",
    "This pooling step reduces the size of the data, making for more efficient calculations. It also helps generalize the ability of the model to recognize certain features. We can capture this generalization by thinking of an example - higher vs lower definition images. If we have a high definition image of something it is extremely clear, and if we have similar objects it is easy to tell them apart. For example if there are multiple cars in an image, we can probably tell them apart pretty easily - the details show the differences. If the cars are in the the background of an image (if they are in the background we are only getting a low definition version of those cars) it is harder to tell them apart - the details are different, but the general \"characteristic\", the fact it is a car, is consistent. Our pooling has the same impact - the pooling changes the higher definition images to lower ones, and we are better able to identify those general characteristics - making it easier to spot things that are \"the same\" in other images. \n",
    "\n",
    "The default pooling window is 2x2, so 4 features are collapsed into one. \n",
    "\n",
    "#### Max and Average Pooling\n",
    "\n",
    "There are two common pooling strategies - max and average. Max pooling takes the maximum value in the pooling window as the output, average takes the average of the values in the pooling window.\n",
    "<ul>\n",
    "<li> Max is more common, it tends to do a better job at finding contrast - differences between light and dark, which is helpful in doing things like separating foreground and background or doing edge detection (think of navigation).\n",
    "<li>Average tends to capture a more broad set of information on the entire image, with less focus on areas of distinct difference. \n",
    "</ul>\n",
    "\n",
    "![Max_Average_Pooling](images/max_avg_pool.png \"Max_Average_Pooling\" )\n",
    "\n",
    "### Normal Neural Network Layers\n",
    "\n",
    "Once the above work is done, potentially with several layers of layers, the final layers in the network are a normal neural network. The CNN parts act to extract features from the image, the final layers take those features and produce a prediction, just as we are used to. So, loosely, the CNN classification model does some image processing in the convolutional layers to extract features and patterns from the image, we then feed the information extracted from the images into a 'normal' neural network model that makes a prediction from some features. \n",
    "\n",
    "### Overal Structure\n",
    "\n",
    "After the entire model is constructed we end up with something like this. The image is translated into a series of representations - one per layer, through the convolutional process. The pooling then lowers the dimensions of those representation, and the process (potentially repeats). At the end of all the convolutional steps we feed our final represenations into the dense stages - these features are in the \"shape\" of an image - with the details being totally different - each layer is a filter (rather than a color) and the dimensions of the \"image\" are determined by the amount of pooling and padding. This is all flattened and the dense part goes on as we are used to in making predictions. \n",
    "\n",
    "![CNN Structure](images/cnn_structure.jpg \"CNN Structure\" )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填充\n",
    "\n",
    "填充是一種設置，用於確定數據的維數是否在卷積層中減少。 我們有兩個選擇：\n",
    "\n",
    "<ul>\n",
    "<li> 有效填充 - 尺寸減小。\n",
    "<li> 相同的填充 - 尺寸保持不變。\n",
    "</ul>\n",
    "\n",
    "通過查看上圖可能最容易說明這一點。 \n",
    "\n",
    "該圖像顯示相同的填充 - 邊界周圍的那些 0 被插入以確保內核可以從邊緣開始並仍然捕獲整個圖片。 \n",
    "\n",
    "如果這是有效的填充，內核將從真正的邊緣開始，而這些邊緣值永遠不會到達圖像的中間。 \n",
    "\n",
    "結果值將比原始值更小。 使用相同的填充可以讓模型更好地捕捉邊界周圍的信息，避免所謂的邊界效應。\n",
    "\n",
    "一般來說，我們應該期望在啟用填充的情況下獲得更好的性能，但代價是一些處理時間和內存。 \n",
    "\n",
    "這種影響通常不是很大，因為對於大多數圖片來說，有價值的東西都在中間——所以影響取決於數據集。 \n",
    "\n",
    "如果您想到 minst 圖像或護照照片之類的東西，邊緣通常並不重要。 \n",
    "\n",
    "如果您想到由自動駕駛汽車拍攝的照片，邊緣可能是最重要的，可以看到路緣和孩子在車前跳躍之類的東西。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 大步前進\n",
    "\n",
    "步幅值是內核窗口每次查看窗口時移動的像素數。 \n",
    "\n",
    "1 的步幅一次移動 1 個像素，較大的步幅“跳過”一些像素。 我們的步幅通常只是 1。\n",
    "\n",
    "### 池化層\n",
    "\n",
    "池化層降低了數據的維度。 在您可能在其他地方聽說過的圖像術語中，\n",
    "\n",
    "我們正在縮減採樣 - 將分辨率較高的內容轉換為較低的分辨率。\n",
    "\n",
    "此池化步驟減少了數據的大小，從而提高了計算效率。 它還有助於概括模型識別某些特徵的能力。 \n",
    "\n",
    "我們可以通過考慮一個例子來捕捉這種概括——高清晰度圖像與低清晰度圖像。 \n",
    "\n",
    "如果我們有某物的高清圖像，它就會非常清晰，如果我們有相似的物體，就很容易將它們區分開來。 \n",
    "\n",
    "例如，如果圖像中有多輛汽車，我們可能可以很容易地將它們區分開來——細節顯示出差異。 \n",
    "\n",
    "如果汽車在圖像的背景中（如果它們在背景中，我們只能得到這些汽車的低清晰度版本），則很難區分它們\n",
    "\n",
    " - 細節不同，但總體“特徵”， 它是汽車的事實是一致的。 \n",
    " \n",
    " 我們的池化具有相同的影響——池化將較高清晰度的圖像更改為較低的圖像，\n",
    " \n",
    " 我們能夠更好地識別這些一般特徵——從而更容易發現其他圖像中“相同”的東西。\n",
    "\n",
    "默認池化窗口為 2x2，因此 4 個特徵合併為一個。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大和平均池化\n",
    "\n",
    "有兩種常見的池化策略——最大和平均。 \n",
    "\n",
    "Max pooling取池化窗口中的最大值作為輸出，average取池化窗口中值的平均值。\n",
    "<ul>\n",
    "<li> Max 更常見，它在發現對比度方面做得更好 - \n",
    "\n",
    "明暗之間的差異，這有助於執行諸如分離前景和背景或進行邊緣檢測（想想導航）之類的事情。\n",
    "\n",
    "<li>平均往往會捕獲整個圖像的更廣泛的信息集，而不太關註明顯差異的區域。\n",
    "</ul>\n",
    "\n",
    "![Max_Average_Pooling](images/max_avg_pool.png \"Max_Average_Pooling\")\n",
    "\n",
    "### 普通神經網絡層\n",
    "\n",
    "完成上述工作後，可能有好幾層，網絡中的最後幾層就是一個普通的神經網絡。 \n",
    "\n",
    "CNN 部分用於從圖像中提取特徵，最後一層採用這些特徵並產生預測，就像我們習慣的那樣。 \n",
    "\n",
    "因此，CNN 分類模型鬆散地在卷積層中進行一些圖像處理以從圖像中提取特徵和模式，\n",
    "\n",
    "然後我們將從圖像中提取的信息提供給“正常”神經網絡模型，該模型根據某些特徵進行預測。\n",
    "\n",
    "### 總體結構\n",
    "\n",
    "在構建了整個模型之後，我們最終得到了這樣的東西。 \n",
    "\n",
    "通過卷積過程，圖像被翻譯成一系列表示——每層一個。 \n",
    "\n",
    "匯集然後降低這些表示的維度，以及過程（可能重複）。 \n",
    "\n",
    "在所有捲積步驟的最後，我們將最終的表徵輸入到密集階段——這些特徵處於圖像的“形狀”中\n",
    "\n",
    "——細節完全不同——每一層都是一個過濾器（而不是一種顏色），並且 “圖像”的尺寸由池化和填充的數量決定。 \n",
    "\n",
    "這一切都被壓平了，密集的部分繼續進行，就像我們在預測時習慣的那樣。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple CNN\n",
    "\n",
    "We can build a simple CNN to make some predictions on our images. In our network we'll have:\n",
    "<ul>\n",
    "<li> Convolutional portion:\n",
    "    <ul>\n",
    "    <li> Convolutional layer - 32 filters, relu activation, same padding. \n",
    "    <li> Pooling layer - (2, 2) size. Results in data 1/4 of original size.\n",
    "    </ul>\n",
    "<li> Dense portion:\n",
    "    <ul>\n",
    "    <li> Flattening of the convolutional results. \n",
    "    <li> Dense layer. \n",
    "    <li> Output layer, 10 classes, softmax prediction. \n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 簡單的CNN\n",
    "\n",
    "我們可以構建一個簡單的 CNN 來對我們的圖像進行一些預測。 在我們的網絡中，我們將擁有：\n",
    "<ul>\n",
    "<li> 卷積部分：\n",
    "     <ul>\n",
    "     <li> 卷積層 - 32 個過濾器，relu 激活，相同的填充。\n",
    "     <li> 池化層 - (2, 2) 大小。 結果數據為原始大小的 1/4。\n",
    "     </ul>\n",
    "<li> 密集部分：\n",
    "     <ul>\n",
    "     <li> 卷積結果的扁平化。\n",
    "     <li> 緻密層。\n",
    "     <li> 輸出層，10 個類別，softmax 預測。\n",
    "     </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Set of Metrics\n",
    "\n",
    "We will define some metrics to watch, then train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Request - Combining Metrics and Labels\n",
    "\n",
    "There might be a more efficient way to do this, metric._name seems like it should return the name of the metric without bothering with the inspection part. It didn't work for me, so I did it this way. I may have had a mistake that I didn't troubelshoot - reader challenge to make it better!\n",
    "\n",
    "<b>Note:</b> the request part was from last year, this is just a note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Results\n",
    "import inspect\n",
    "metric_list_fin =[]\n",
    "for met in metric_list:\n",
    "    atts = inspect.getmembers(met)\n",
    "    tmp_name = [a for a in atts if(a[0] == \"_name\")]\n",
    "    metric_list_fin.append(tmp_name[0][1])\n",
    "\n",
    "metric_list_fin.insert(0,\"loss\")\n",
    "metric_df = pd.DataFrame(\n",
    "    {'Metrics': metric_list_fin,\n",
    "     'Train': train_eval,\n",
    "     'Test': test_eval\n",
    "    })\n",
    "metric_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interim Results\n",
    "\n",
    "We haven't really done all that much to tune our new model, but in the tests that I ran we were usually seeing somewhere around a 70% validation accuracy at this point - over 20% better than what we got with a non-convolutional model. This is the power of the convolutional layers - they are able to extract features from the images that are useful for making predictions in a way that a \"normal\" model can't. They also do this without requiring any real insight from us - we don't have to tell the model what features to look for, it figures it out on its own, so we don't have to be incredibly knowledgeable about images we are dealing with to make a good model.\n",
    "\n",
    "<b>Note:</b> our results are likely to be a little worse than expected below due to early stopping, if we made the delta a little more permissive, we'd likely see some of the models get a little better slowly. That's what happened in inital trials, but it can take a long time to run, so we compromise. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Trials\n",
    "\n",
    "We'll try a few different CNN models, and see how they perform. The steps here are broadly pointing towards a more accurate model, but not super specifically. We are trying a few things here to see what we get. By the end, we'll likely have an overall more accurate model, we hope...\n",
    "\n",
    "#### Multiple Convolutional Layers\n",
    "\n",
    "We can try several convolutional layers, and see if that improves our results. We'll add a couple more convolutional layers, and corresponding pooling layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多個卷積層\n",
    "\n",
    "我們可以嘗試幾個卷積層，看看是否能改善我們的結果。 我們將添加更多的捲積層和相應的池化層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs - Convolutional Layers, Padding, Kernel Size\n",
    "\n",
    "We can change the main options for our CNN without much difficulty. These changes will have impacts on our model similar to changes that we can make to any sets of hyperparameters in any models. Some changes will make minor diffrences, other large ones, and some will have no impact at all. The impact depends on the dataset that we are dealing with, and the strucutre of the model we are using. We have some intuition that we can use to guide trials, such as setting the padding depending on if the edges of our image are valuable or not, but for the most part this is trial and error land. \n",
    "\n",
    "#### Padding\n",
    "\n",
    "The default padding is valid, we set ours to same up above. With padding set to same our models should do a better job of capturing information around the edges of the image. We can see what the results are when we revert it to valid. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 填充\n",
    "\n",
    "默認填充是有效的，我們將其設置為與上面相同。 將填充設置為相同時，我們的模型應該可以更好地捕獲圖像邊緣周圍的信息。 當我們將其恢復為有效時，我們可以看到結果是什麼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Size\n",
    "\n",
    "We can also modify the kernel size. The kernel size is the size of the \"window\" - or how many pixels the filter looks at each time. 3x3 and 5x5 are probably the most common, for larger images sometimes something larger is used. The kernel size is pretty much always odd, so it is symetrical around the center pixel, this isn't a requirement, but it is a common practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 內核大小\n",
    "\n",
    "我們還可以修改內核大小。 內核大小是“窗口”的大小 - 或者過濾器每次查看多少像素。 3x3 和 5x5 可能是最常見的，對於較大的圖像，有時會使用更大的圖像。 \n",
    "\n",
    "內核大小幾乎總是奇數，因此它圍繞中心像素對稱，這不是必需的，但這是一種常見的做法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Size\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5,5), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (5,5), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (5,5), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers \n",
    "\n",
    "Just like with a regular neural network we can change the number of layers. In order to allow for many layers to exist despite the pooling, we will need to increase the number of filters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpooled Convolutional Layers\n",
    "\n",
    "We can also stack convolutional layers without pooling in between. This will allow us to capture more information, but will also increase the size of our data. We can try to double up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "It is also relatively common to try batch normalization - or normalization layers applied between layers of a network. Batch normalization tends to have two main impacts - increase in model stability and acceleration of convergance. The reason for this is because each time data is transformed the inputs to the next layer can have their distribution shifted - something called internal covariate shift. Batch normalization adjusts this by renormalizing in the middle of each step. Note also that here we separate the dense layer and the activation - as we did with the from-scratch version. Batch normalization also can allow for faster learning rates in many cases - the improved convergance lets the algorithm go faster. \n",
    "\n",
    "Batch normalization is relatively new, put forth in around 2015, and the exact nature of how it improves things mathmatically is still debated (which surprised me). In particular, there is still debate on if it should be inserted between the regular layer and the activation as we have done here, or applied post activation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批量歸一化\n",
    "\n",
    "嘗試批量歸一化或在網絡層之間應用歸一化層也相對常見。 批量歸一化往往有兩個主要影響——增加模型穩定性和加速收斂。 \n",
    "\n",
    "這樣做的原因是因為每次轉換數據時，下一層的輸入都會改變它們的分佈——這稱為內部協變量偏移。 批量歸一化通過在每個步驟的中間重新歸一化來調整這一點。 \n",
    "\n",
    "另請注意，這裡我們將密集層和激活層分開——就像我們對從頭開始的版本所做的那樣。 \n",
    "\n",
    "在許多情況下，批量歸一化還可以實現更快的學習率——改進的收斂性讓算法運行得更快。\n",
    "\n",
    "批量歸一化相對較新，大約在 2015 年提出，它如何在數學上改進事物的確切性質仍在爭論中（這讓我感到驚訝）。 \n",
    "\n",
    "特別是，關於是否應該像我們在這裡所做的那樣將其插入常規層和激活層之間，或者應用後激活層，仍然存在爭論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (3, 3), input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(BatchNormalization(epsilon=.1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=\"l2\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Configuration Results\n",
    "\n",
    "As we can see, the convolutional neural networks give us models that seem to be much more capable of making predictions on our images. In most of my trials, the most accurate models got to roughly 80% accuracy on the validation data, which is not bad, considering a regular model is sub 50%. Down below, after a surprise, we'll combine a bunch of these things together and see what we can get. \n",
    "\n",
    "<b>Note:</b> in a few trials, the batch normalization example above tended to be most impacted by setting the early stopping. If we let it run, the accuracy has seemed to improve, but slowly and with a lot of variation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories of Images\n",
    "\n",
    "We are going to grab some data, but this time it is not getting loaded into some data structure in our application, the file is being saved to disk and uncompressed. The end result of the code below is the same as if you were to download a file and unzip it (or in this case, un-tarball it). \n",
    "\n",
    "Processing images in a way similar to this is common - we may have a large number of images, and we don't want to load them all into memory at once. We can also use this approach to load images from a database, or from a remote location. The idea of loading the data into datasets is basically the same, but the details can vary a fair bit. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 圖像目錄\n",
    "\n",
    "我們要抓取一些數據，但這次它沒有加載到我們應用程序的某些數據結構中，文件被保存到磁盤並解壓縮。 \n",
    "\n",
    "下面代碼的最終結果與下載文件並解壓縮（或在本例中為解壓包）一樣。\n",
    "\n",
    "以類似的方式處理圖像很常見——我們可能有大量圖像，我們不想一次將它們全部加載到內存中。 我們還可以使用這種方法從數據庫或遠程位置加載圖像。 \n",
    "\n",
    "將數據加載到數據集中的想法基本相同，但細節可能會有所不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import PIL \n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                                   fname='flower_photos',\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Download by Printing One Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "PIL.Image.open(str(roses[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "When dealing with things like images we commonly have actual images, not arrays or dataframes. Keras has a preprocessing function to take a folder of images and automatically create a dataset from it. A dataset is a built in datatype in tensorflow, it is kind of a specialized type of data structure that is meant to store larger volumes of generally non-tabular data, and is purpose made to be put through tensorflow networks. Here we will basically have the image files on disk be automatically loaded and split into two datasets - training and validation. When fitting the model we can use this dataset just as we would an array. \n",
    "\n",
    "This type of setup is fairly common when dealing with images. The particular function we used here - image_dataset_from_directory - does bulk data loading from the file structure on disk, handling all of the I/O details on its own. \n",
    "\n",
    "#### Dataset Components\n",
    "\n",
    "The dataset is basically the data itself, along with some extra information on how it is to be used. For example, the data and targets are both in the dataset, the validation split is preset, as is the batch size. \n",
    "\n",
    "<b>Note:</b> in larger scale applications the batch size may be constrained by the memory size of the GPU. We <i>don't</i> want to load more data that can fit into memory under any circumstance, as that means that the processor will need to wait for data to be loaded to and from main memory or disk, which is very slow. For us, this isn't a pressing concern, but it is something to keep in mind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數據集\n",
    "\n",
    "在處理圖像之類的東西時，我們通常有實際的圖像，而不是數組或數據幀。 Keras 有一個預處理功能，可以獲取圖像文件夾並從中自動創建數據集。 \n",
    "\n",
    "數據集是 tensorflow 中的內置數據類型，它是一種特殊類型的數據結構，旨在存儲大量通常非表格數據，並且旨在通過 tensorflow 網絡進行傳輸。 \n",
    "\n",
    "在這裡，我們基本上會讓磁盤上的圖像文件自動加載並分成兩個數據集——訓練和驗證。 在擬合模型時，我們可以像使用數組一樣使用此數據集。\n",
    "\n",
    "這種類型的設置在處理圖像時相當常見。 我們在這裡使用的特定函數 - image_dataset_from_directory - 從磁盤上的文件結構加載大量數據，自行處理所有 I/O 細節。\n",
    "\n",
    "#### 數據集組件\n",
    "\n",
    "數據集基本上就是數據本身，以及一些關於如何使用它的額外信息。 例如，數據和目標都在數據集中，驗證拆分是預設的，批量大小也是如此。\n",
    "\n",
    "<b>注意</b>：在更大規模的應用程序中，批量大小可能會受到 GPU 內存大小的限制。 \n",
    "\n",
    "我們<i>不想</i>在任何情況下都希望加載更多可以裝入內存的數據，因為這意味著處理器將需要等待數據加載到主內存或磁盤或從主內存或磁盤加載數據，這是 非常慢。 對我們來說，這不是一個緊迫的問題，但需要牢記在心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flowers\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Model with the Datasets\n",
    "\n",
    "For the most part, things don't change significantly when using the datasets. We get to drop the arguments that are embedded in the dataset itself, like how much data to hold for validation. \n",
    "\n",
    "#### Example of Different Outputs\n",
    "\n",
    "This model also has an example of an odd setup for its output. We probably don't want to do this in general, but it is worth seeing. Even though we are doing a classification, there is no activation on the output. As well, the loss function has a parameter \"from logits\" set to true. This is because the output is not a probability, but a raw value. This is basically offloading the softmax step to be done outside of the model's layers, because we can get output to the raw predicitons. For us, this likely won't be all that critical, it is something that you may see. One reason to do this is because in certain scenarios, the calculations may be more stable, easing convergance. Other than that edge case, we can treat it as interchangable, but probably less user friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=\"adam\", metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"))\n",
    "train_log = model.fit(train_ds, epochs=10, verbose=1, callbacks=[callback], validation_data=val_ds)\n",
    "train_eval = model.evaluate(train_ds)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "When using images we can employ data augmentation to increase the size of our dataset, allowing for better training and resulting in better models. More data is king when it comes to model quality, so this is very helpful. \n",
    "\n",
    "The reason data augmentation is common and easy with images, where it isn't as straightforward with structured data is due to the nature of an image and what we are generally trying to do with it. Image problems are generally things like recognition - identifying what is in an image. If we are looking to identify if there is a cat in an image we probably don't  care if the cat is on the left side, the right side, rotated in any direction, tilted, etc... cats move in stupid and random ways, because they're insane lunatics, all we care about is if the cat is somewhere in there. \n",
    "\n",
    "![Augmented Cat](images/cat_aug.png \"Augmented Cat\" )\n",
    "\n",
    "We can take advantage of this by doing all of those transformations to our images and using those transformed copies to augment our dataset! All of the mirrored, rotated, shifted, etc... images are just as good for the purposes of detecting a cat in an image, so we can use them. Free data!\n",
    "\n",
    "In practice this is common and keras makes it quite easy. We can create a mini-network and apply some transformations, then just stick this into the top of our model. Augmentation, when it makes sense like this, has few to no downsides. We can expect a more generalizable model as if our goal is to spot cats, being able to spot them on the left, or on the right, or shifted, or rotated is an actual thing that we directly want our model to be able to do. \n",
    "\n",
    "Augmenting data and generating training data is something that is relatively common in many areas of machine learning - gathering and storing data can be expensive, so we want to make the most of what we have. The image examples are probably the most easy to understand, but the same idea applies to other types of data. The creation of training data is obviously something that is highly application dependent, and follows a simple idea that we've touched on a few times - as long as it helps the model predict what we need, it doesn't matter what the training data actually looks like. We've seen this in things like doing a log-transformation on income, or grouping infrequent categories into \"other\", the only goal is to help the model, and we can sometimes do that by distorting the data.\n",
    "\n",
    "### Image Generators\n",
    "\n",
    "As well, we can also introduce image generators, which can also help quite a bit in preparing our data. Image generators are a way to create a dataset from a directory of images, but with some extra functionality. We can use them to apply transformations to the images, and we can also use them to apply some preprocessing to the images. Here, we can do some augemntation. These generators are like a \"smart\" version of a dataset, more specifically they take in the data, potentially do some things with it, then generate the data as output on demand. Here we will ask each one to generate some images with the transformations that we setup in the generator as augmentation steps. \n",
    "\n",
    "One thing to note here is that I have a seemingly redundant piece of pulling data. This is because some of the augmentation functions depend on knowing the data, such as normalization, which needs to know the range of values that it is normalizing. This is why we pull some data, use it to set those parameters, then pull the data again to actually create the datasets. This is weird, and poorly designed - I thought that this really wouldn't be the correct way to do it, but it looks like this is a bug (or poor design choice) in the newest version of tensorflow. There are a lot of choices of things that we can do to augment, the tensorflow documentation has a good list of them here: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator (I commented these out as it makes for more printable images if we don't use them)\n",
    "\n",
    "<b>Big Note:</b> In my trials I got a few warning messages that were along the lines of \"there is no registered converter\", along with a note about a while loop. As far as I can tell this is a bug in a newer version of tensorflow (it didn't happen when I wrote this). These warnings mean that the process will be slow, but things should otherwise work. I will assume that at some point this error will be fixed. For some more info: https://stackoverflow.com/questions/73304934/\n",
    "\n",
    "As far as I can see there are two workarounds, one is to downgrade tensorlfow, which I wouldn't reccomend on your python environment as it can be annoying, you could make another environment, your \"Python_augmenting_env\", and install the desired version there. The other is to basically rewrite the augmentation code, which is a bit of a pain. For us, we can try a specific import in Colab, since the environment is temporary, we don't need to worry about breaking anything else. I'm not going to put this in the code, because I don't want to cause any issues accidentally, but we can manually place it in the code if we desire. \n",
    "\n",
    "tensorflow-data-augmentation-gives-a-warning-using-a-while-loop-for-converting even the Tensorflow documentation example for image classification gets hit:\n",
    "\n",
    "![Augmentation Warning](images/augment_error.png \"Augmentation Warning\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 圖像生成器\n",
    "\n",
    "同樣，我們還可以引入圖像生成器，這對準備數據也有很大幫助。 圖像生成器是一種從圖像目錄創建數據集的方法，但具有一些額外的功能。 \n",
    "\n",
    "我們可以使用它們對圖像進行變換，也可以使用它們對圖像進行一些預處理。 在這裡，我們可以做一些增強。 \n",
    "\n",
    "這些生成器就像數據集的“智能”版本，更具體地說，它們接收數據，可能用它做一些事情，然後按需生成數據作為輸出。 \n",
    "\n",
    "在這裡，我們將要求每個人使用我們在生成器中設置的轉換作為增強步驟來生成一些圖像。\n",
    "\n",
    "這裡需要注意的一點是，我有一段看似多餘的拉取數據。 這是因為一些增強函數依賴於了解數據，例如規範化，它需要知道它正在規範化的值的範圍。 \n",
    "\n",
    "這就是為什麼我們提取一些數據，使用它來設置這些參數，然後再次提取數據以實際創建數據集的原因。 \n",
    "\n",
    "這很奇怪，而且設計得很糟糕——我認為這真的不是正確的方法，但看起來這是最新版本的 tensorflow 中的一個錯誤（或糟糕的設計選擇）。 \n",
    "\n",
    "我們可以做很多選擇來增強，張量流文檔在這裡有一個很好的列表：\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator（ 我把這些註釋掉了，因為如果我們不使用它們，它會產生更多可打印的圖像）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAugmenter = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    #fill_mode='nearest',\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2, \n",
    "    rescale=1./255,\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "valAugmenter = ImageDataGenerator(\n",
    "    validation_split = 0.2,\n",
    "    rescale=1./255\n",
    "    )\n",
    "\n",
    "# Pull Data to train the augmentation\n",
    "fitAugmenter = ImageDataGenerator(\n",
    "    validation_split = 0.1,\n",
    "    rescale=1./255\n",
    "    )\n",
    "fitAug = fitAugmenter.flow_from_directory(data_dir, seed=33, subset=\"validation\", batch_size=BATCH_SIZE, class_mode=\"categorical\",)\n",
    "trainAugmenter.fit(fitAug.next()[0])\n",
    "\n",
    "augTrain = trainAugmenter.flow_from_directory(data_dir, seed=33, subset=\"training\", batch_size=BATCH_SIZE, class_mode=\"categorical\",)\n",
    "augVal = valAugmenter.flow_from_directory(data_dir, seed=33, subset=\"validation\", batch_size=BATCH_SIZE, class_mode=\"categorical\",)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preview Image\n",
    "\n",
    "The generator generates files, on demand. In our context, we basically load the data into the generator, use the generator \"as\" the dataset, and the generator will generate the data, along with the augmentation, on demand when we fit the model. We can pull one image and print it to see what we get from the generator. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 預覽圖片\n",
    "\n",
    "生成器按需生成文件。 在我們的上下文中，我們基本上將數據加載到生成器中，將生成器“用作”數據集，生成器將在我們擬合模型時按需生成數據以及增強。 我們可以拉取一張圖像並打印出來，看看我們從生成器中得到了什麼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp, ytmp = augTrain.next()\n",
    "for i in range(0,4):\n",
    "    image = tmp[i]\n",
    "    label = ytmp[i]\n",
    "    print (label)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Using the image generators is pretty simple. The genearators \"become\" the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(augment)\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "train_log = model.fit(augTrain, epochs=BASE_EPOCHS, verbose=1, callbacks=[callback], validation_data=augVal)\n",
    "\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the original dataset from the first section and build a model incorporating some of the CNN features. Add data augmentation, then manipulate things such as the number of layers, kernel size, padding, dropouts, etc... to try to improve accuracy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Reasoning on What to Try\n",
    "\n",
    "In the examples above, the training accuracy generally got quite good, and in the earlier models where we didn't do as many changes, there was a large gap between testing and training accuracy. This gap lessened as we added regularization and dropouts, so that's a good sign that the model was overfitting. Out model likely has enough capacity with about 2 hidden layers. This doesn't necissarily mean that we should stop there, but we can pretty safely thing that is \"enough\" in terms of the capacity of the model. If we add more layers it isn't to make the model able to learn the data, we've already gotten there. We have also seen that expanding the number of filters in a convolutional layer can help, so we can add that in. As well, we can augment the data, which should help in general.\n",
    "\n",
    "<b>Note:</b> the broken augmentation puts a bit of a damper on this one, as it is very slow to run trials. We should be able to add augemntation to one of the model configurations that got around 80% accuracy above, and see the overfit gap lessen, potentially leaving some room to allow for more fitting - either more epochs or more capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "trainDatagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "    fill_mode='nearest',\n",
    "    width_shift_range=0.2,\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "valDatagen = ImageDataGenerator(\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "# We have the data, so we can fit to it. \n",
    "trainDatagen.fit(X_train)\n",
    "\n",
    "train_generator = trainDatagen.flow(X_train, y_train, batch_size=BATCH_SIZE, subset='training', seed=55)\n",
    "validation_generator = valDatagen.flow(X_train, y_train, batch_size=BATCH_SIZE, subset='validation', seed=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), input_shape=(32, 32, 3), padding=\"same\", kernel_regularizer=\"l2\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=\"l2\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=\"l2\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=\"l2\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(train_generator,\n",
    "                      validation_data=validation_generator,\n",
    "                      epochs=BASE_EPOCHS, \n",
    "                      verbose=1, \n",
    "                      callbacks=[callback],\n",
    "                      workers=4)\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
