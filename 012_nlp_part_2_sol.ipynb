{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More NLP\n",
    "\n",
    "## Truncated Singular Value Decomposition and Dimensionality Reduction\n",
    "\n",
    "When processing text we end up with feature sets that are large! There is up to one feature per different word in our text sample, as well as more for multi-word combinations if there are larger ngrams allowed, far larger than a typical feature set that we're used to. One thing we can do when vectorizing is just to cap the number of features we end up with, but that doesn't seem to be the most sophisticated or smartest approach. \n",
    "\n",
    "TSVD is one thing that we can do to chop down the feature set - or reduce the dimensions - with a little more thought. \n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a common technique in machine learning, it does its name - reduces the dimensions in our feature data. We often want to do this for several reasons: \n",
    "<ul>\n",
    "<li> To reduce the amount of time it takes to train a model.\n",
    "<li> To reduce the amount of memory required to store the data.\n",
    "<li> To reduce the amount of noise in the data.\n",
    "<li> To make the data more interpretable.\n",
    "<li> To make the data more amenable to visualization.\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更多自然語言處理\n",
    "\n",
    "## 截斷奇異值分解和降維\n",
    "\n",
    "在處理文本時，我們最終得到的特徵集很大！ 在我們的文本樣本中，每個不同的單詞最多有一個特徵，如果允許使用更大的 ngram，則多詞組合還有更多特徵，這遠大於我們習慣的典型特徵集。 矢量化時我們可以做的一件事就是限制我們最終得到的特徵數量，但這似乎不是最複雜或最聰明的方法。\n",
    "\n",
    "TSVD 是我們可以做的一件事，可以通過更多的思考來減少特徵集——或減少維度。\n",
    "\n",
    "## 降維\n",
    "\n",
    "降維是機器學習中的一種常用技術，正如它的名字一樣——降低特徵數據的維度。 我們經常出於以下幾個原因想要這樣做：\n",
    "<ul>\n",
    "<li> 減少訓練模型所需的時間。\n",
    "<li> 減少存儲數據所需的內存量。\n",
    "<li> 減少數據中的噪聲量。\n",
    "<li> 使數據更具可解釋性。\n",
    "<li> 使數據更易於可視化。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset from Last Time\n",
    "\n",
    "We'll load the spam dataset and vectorize it with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 89635)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 easter</th>\n",
       "      <th>00 easter prize</th>\n",
       "      <th>00 easter prize draw</th>\n",
       "      <th>00 sub</th>\n",
       "      <th>00 sub 16</th>\n",
       "      <th>00 sub 16 remove</th>\n",
       "      <th>00 sub 16 unsub</th>\n",
       "      <th>00 subs</th>\n",
       "      <th>00 subs 16</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom cine actually</th>\n",
       "      <th>zoom cine actually tonight</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zouk nichols</th>\n",
       "      <th>zouk nichols paris</th>\n",
       "      <th>zouk nichols paris free</th>\n",
       "      <th>zyada</th>\n",
       "      <th>zyada kisi</th>\n",
       "      <th>zyada kisi ko</th>\n",
       "      <th>zyada kisi ko kuch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 easter  00 easter prize  00 easter prize draw  00 sub  \\\n",
       "2558  0.0        0.0              0.0                   0.0     0.0   \n",
       "2208  0.0        0.0              0.0                   0.0     0.0   \n",
       "5103  0.0        0.0              0.0                   0.0     0.0   \n",
       "718   0.0        0.0              0.0                   0.0     0.0   \n",
       "3296  0.0        0.0              0.0                   0.0     0.0   \n",
       "\n",
       "      00 sub 16  00 sub 16 remove  00 sub 16 unsub  00 subs  00 subs 16  ...  \\\n",
       "2558        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "2208        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "5103        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "718         0.0               0.0              0.0      0.0         0.0  ...   \n",
       "3296        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "\n",
       "      zoom cine actually  zoom cine actually tonight  zouk  zouk nichols  \\\n",
       "2558                 0.0                         0.0   0.0           0.0   \n",
       "2208                 0.0                         0.0   0.0           0.0   \n",
       "5103                 0.0                         0.0   0.0           0.0   \n",
       "718                  0.0                         0.0   0.0           0.0   \n",
       "3296                 0.0                         0.0   0.0           0.0   \n",
       "\n",
       "      zouk nichols paris  zouk nichols paris free  zyada  zyada kisi  \\\n",
       "2558                 0.0                      0.0    0.0         0.0   \n",
       "2208                 0.0                      0.0    0.0         0.0   \n",
       "5103                 0.0                      0.0    0.0         0.0   \n",
       "718                  0.0                      0.0    0.0         0.0   \n",
       "3296                 0.0                      0.0    0.0         0.0   \n",
       "\n",
       "      zyada kisi ko  zyada kisi ko kuch  \n",
       "2558            0.0                 0.0  \n",
       "2208            0.0                 0.0  \n",
       "5103            0.0                 0.0  \n",
       "718             0.0                 0.0  \n",
       "3296            0.0                 0.0  \n",
       "\n",
       "[5 rows x 89635 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.drop(columns={\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"}, inplace=True)\n",
    "df.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)\n",
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp = vec_tf.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_tf.get_feature_names()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "tok_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA - Latent Semantic Analysis\n",
    "\n",
    "The TSVD performs somehting called latent semantic analysis. The process of LSA and the math behind it are not something we need to explore in detail. (LSA is often called LSI - Latent Semantic Indexing). The idea of LSA is that it can generate \"concepts\" in the text. These concepts are found by looking at which terms occur in which documents - documents that have the same terms repeated are likely related to the same concept; other documents that share other words with those documents are likely on the same concept as well.  \n",
    "\n",
    "An important part is the word \"Latent\" - i.e. the patterns detected are hidden, not explicit in the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA——潛在語義分析\n",
    "\n",
    "TSVD 執行一些稱為潛在語義分析的事情。 LSA 的過程及其背後的數學原理不是我們需要詳細探討的。\n",
    "\n",
    "LSA 通常稱為 LSI——潛在語義索引）。 LSA的思想是可以在文本中生成“概念”。 \n",
    "\n",
    "這些概念是通過查看哪些術語出現在哪些文檔中找到的——重複具有相同術語的文檔可能與相同的概念相關； \n",
    "\n",
    "與這些文檔共享其他詞的其他文檔也可能在相同的概念上。\n",
    "\n",
    "一個重要的部分是“潛在”這個詞——即檢測到的模式是隱藏的，而不是在數據中顯式的。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement SVD to Trim Dataset\n",
    "\n",
    "We are starting with LOTS of feature inputs. Below we can loop through several models of different number of remaining components to see the accuracy depending on the number of features we keep in the feature set. The truncated part of truncated SVD trims the featureset down to the most significant features. \n",
    "\n",
    "We started with a lot of features - we can make predictions that are close to as accurate with far fewer, hopefully!\n",
    "\n",
    "<b>Note:</b> this might take a long time to run, depending on your computer. Change the \"for i in range()\" part to cut down on the number of iterations to make it run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor i in range(1,15):\n",
    "\t\tn = i*10\n",
    "\t\tsteps = [('svd', TruncatedSVD(n_components=n)), ('m', LinearSVC(max_iter=100, tol=.01))]\n",
    "\t\tmodels[str(n)] = Pipeline(steps=steps)\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t#Splits cut for speed\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=4, n_repeats=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, tok_df[0:1000], y[0:1000])\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "# plot model performance for comparison\n",
    "\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp_vec = tf_idf.fit_transform(df[\"text\"])\n",
    "\n",
    "tok_cols2 = tf_idf.get_feature_names()\n",
    "tmp_df = pd.DataFrame(tmp_vec.toarray(), columns=tok_cols2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1608    Are your freezing ? Are you home yet ? Will yo...\n",
      "5101            Nope thats fine. I might have a nap tho! \n",
      "3757     Lol no ouch but wish i'd stayed out a bit longer\n",
      "1554                      Yeah we do totes. When u wanna?\n",
      "2484    Only if you promise your getting out as SOON a...\n",
      "1864                  You call him now ok i said call him\n",
      "1729                   Lol yeah at this point I guess not\n",
      "2710    Hope you enjoyed your new content. text stop t...\n",
      "2909       How do you guys go to see movies on your side.\n",
      "4951    Welcome to Select, an O2 service with added be...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(tmp_df, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy\n",
    "\n",
    "Since we are planning on dropping a bunch of data, we can try a model first to see what the baseline accuracy is. I'm also going to limit the number of features here, since using the entire dataset will take ages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9619526202440776"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_base = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\", max_features=2000)\n",
    "tmp_vec_base = tf_idf_base.fit_transform(df[\"text\"])\n",
    "\n",
    "tok_cols2_base = tf_idf_base.get_feature_names()\n",
    "tmp_df_base = pd.DataFrame(tmp_vec_base.toarray(), columns=tok_cols2_base)\n",
    "X_tr_base, X_te_base, y_tr_base, y_te_base = train_test_split(tmp_df_base, y)\n",
    "\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr_base, y_tr_base)\n",
    "pipe_test.score(X_te_base, y_te_base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement LSA and Model\n",
    "\n",
    "We can use the truncated SVD to reduce the number of features in our dataset, in much the way we'd use any other data preparation step in a pipeline. \n",
    "\n",
    "#### 實現 LSA 和模型\n",
    "\n",
    "我們可以使用截斷的 SVD 來減少數據集中的特徵數量，就像我們在管道中使用任何其他數據準備步驟一樣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_tmp = TruncatedSVD(n_components=80)\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"svd\", svd_tmp), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr, y_tr)\n",
    "pipe_test.score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but TruncatedSVD was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000211</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>-0.000319</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001577</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>-0.000756</td>\n",
       "      <td>-0.001533</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>-0.000680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000210</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>-0.000435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002048</td>\n",
       "      <td>-0.001105</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>-0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>0.013732</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>0.039309</td>\n",
       "      <td>0.026615</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>-0.017715</td>\n",
       "      <td>0.034112</td>\n",
       "      <td>0.033260</td>\n",
       "      <td>0.050616</td>\n",
       "      <td>0.029373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000406</td>\n",
       "      <td>-0.000480</td>\n",
       "      <td>-0.000663</td>\n",
       "      <td>-0.000628</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028930</td>\n",
       "      <td>0.015672</td>\n",
       "      <td>0.028103</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.054079</td>\n",
       "      <td>-0.004065</td>\n",
       "      <td>-0.007986</td>\n",
       "      <td>-0.022096</td>\n",
       "      <td>-0.004089</td>\n",
       "      <td>0.031915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>-0.000947</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>-0.000756</td>\n",
       "      <td>-0.000923</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>-0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000243</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>-0.000302</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>-0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>-0.000549</td>\n",
       "      <td>-0.000446</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.003462</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000555</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000983</td>\n",
       "      <td>-0.001722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002786</td>\n",
       "      <td>0.008854</td>\n",
       "      <td>-0.003377</td>\n",
       "      <td>0.019004</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>-0.008197</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>-0.034245</td>\n",
       "      <td>-0.022086</td>\n",
       "      <td>0.017918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.000247</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>-0.002459</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>-0.001878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>-0.000126</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000247</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>-0.000373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.000211 -0.000159 -0.000228 -0.000185 -0.000163 -0.000221 -0.000277   \n",
       "1    -0.000210 -0.000213 -0.000315 -0.000240 -0.000248 -0.000292 -0.000439   \n",
       "2     0.000909  0.001324  0.002129  0.000183  0.000883 -0.000123  0.004945   \n",
       "3    -0.000406 -0.000480 -0.000663 -0.000628 -0.000767  0.000620 -0.000370   \n",
       "4    -0.000128 -0.000124 -0.000160  0.002172 -0.000124  0.001447 -0.000157   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4174 -0.000125 -0.000122 -0.000181  0.000202 -0.000128 -0.000153 -0.000243   \n",
       "4175 -0.000061 -0.000060 -0.000088 -0.000070 -0.000071 -0.000071 -0.000132   \n",
       "4176  0.003462 -0.000285 -0.000030 -0.000555 -0.000045  0.000141 -0.000371   \n",
       "4177 -0.000168 -0.000162 -0.000247 -0.000199 -0.000195 -0.000160 -0.000342   \n",
       "4178 -0.000126  0.000488 -0.000167 -0.000126  0.000402 -0.000077 -0.000247   \n",
       "\n",
       "            7         8         9   ...        70        71        72  \\\n",
       "0    -0.000215 -0.000319 -0.000106  ... -0.001577 -0.000032 -0.000504   \n",
       "1    -0.000254 -0.000344 -0.000435  ... -0.002048 -0.001105  0.000259   \n",
       "2    -0.001406  0.013732  0.005316  ...  0.000380  0.015491  0.039309   \n",
       "3     0.000460  0.002797 -0.000154  ...  0.028930  0.015672  0.028103   \n",
       "4    -0.000151 -0.000166 -0.000168  ...  0.000699 -0.000947  0.000181   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4174 -0.000092 -0.000219 -0.000113  ... -0.001117 -0.000475  0.000131   \n",
       "4175 -0.000051 -0.000138 -0.000043  ... -0.000281 -0.000549 -0.000446   \n",
       "4176 -0.000172 -0.000983 -0.001722  ... -0.002786  0.008854 -0.003377   \n",
       "4177 -0.000148 -0.000320 -0.000322  ...  0.000636 -0.002459 -0.001586   \n",
       "4178 -0.000155 -0.000124  0.000469  ... -0.000736 -0.000230 -0.000080   \n",
       "\n",
       "            73        74        75        76        77        78        79  \n",
       "0    -0.000756 -0.001533 -0.000253  0.001000  0.000505  0.002697 -0.000680  \n",
       "1     0.000828  0.001284  0.000622  0.001240 -0.000636  0.001016 -0.000855  \n",
       "2     0.026615  0.000512 -0.017715  0.034112  0.033260  0.050616  0.029373  \n",
       "3     0.003465  0.054079 -0.004065 -0.007986 -0.022096 -0.004089  0.031915  \n",
       "4    -0.000460 -0.000756 -0.000923  0.002021 -0.001224  0.002820 -0.000239  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4174 -0.000221  0.000493 -0.000302  0.000592  0.000779 -0.000693 -0.000270  \n",
       "4175 -0.001047  0.000390  0.001535  0.000723  0.000175  0.000516  0.001108  \n",
       "4176  0.019004  0.013862 -0.008197  0.001502 -0.034245 -0.022086  0.017918  \n",
       "4177  0.001833  0.000297 -0.000121  0.002744  0.001395  0.002395 -0.001878  \n",
       "4178 -0.000394  0.000368 -0.000699 -0.000523  0.000948  0.000939 -0.000373  \n",
       "\n",
       "[4179 rows x 80 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svd_tmp.transform(X_tr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA Results\n",
    "\n",
    "In the second model, our feature set is far smaller, but we're still getting a very high accuracy. If the original dataset that we started with was very large, this impact would be magnified greatly. In general, NLP models use a lot of data, so this dimesionality reduction can help reduce training datasets that are massive and may even be impractical to process. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA 結果\n",
    "\n",
    "在第二個模型中，我們的特徵集要小得多，但我們仍然可以獲得非常高的準確性。 \n",
    "\n",
    "如果我們開始的原始數據集非常大，這種影響會被大大放大。 \n",
    "\n",
    "通常，NLP 模型使用大量數據，因此這種降維有助於減少海量甚至可能無法處理的訓練數據集。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topics\n",
    "\n",
    "One of the things that LSA can do is to find \"topics\" in the text. We can use the components of the SVD to find the most important words in each topic. A \"topic\" is something that is not explicitly stated in the text, but is implied by the words that are used - if we have several documents that tend to use the same words, they are likely to be about the same topic. The LSA process is able to look for these cooccuring words and the documents that contain them, and group them as being about the same topic. The mechanics of this are some matrix math that is beyond what we neeed to know, but we can picture it like this.\n",
    "\n",
    "![LSA Math](images/lsa_math.webp \"LSA Math\")\n",
    "\n",
    "The topic extraction is also an example of unsupervised learning - something we'll look at more soon with clustering. We don't provide the topics to the mode in advance like we woud with a normal classification - we just give the LSA process the data, and it figures it out on its own.\n",
    "\n",
    "The model doesn't \"understand\" what each topic is, but it is able to pick up on trends of tokens that tend to occur together in documents. Text that contains \"ball\", \"game\", \"football\", \"play\", \"quarterback\" is likely to be about football - the model won't know it is football, but it will know that those words tend to occur together, and documetns that contain those words are likely to be about the same topic. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＃＃＃＃＃ 主題\n",
    "\n",
    "LSA 可以做的一件事是在文本中找到“主題”。 我們可以使用 SVD 的組件來找到每個主題中最重要的單詞。 \n",
    "\n",
    "“主題”是文本中未明確說明的內容，但被所使用的詞語所暗示——如果我們有幾份文檔傾向於使用相同的詞語，那麼它們很可能是關於同一主題的。 \n",
    "\n",
    "LSA 過程能夠查找這些同時出現的詞和包含它們的文檔，並將它們分組為關於同一主題。 \n",
    "\n",
    "它的機制是一些矩陣數學，超出了我們需要知道的範圍，但我們可以這樣描繪它。\n",
    "\n",
    "主題提取也是無監督學習的一個例子——我們很快就會通過聚類來了解這一點。 \n",
    "\n",
    "我們不會像正常分類那樣提前向模式提供主題——我們只是給 LSA 處理數據，它會自己計算出來。\n",
    "\n",
    "該模型不“理解”每個主題是什麼，但它能夠了解文檔中往往一起出現的標記趨勢。 \n",
    "\n",
    "包含“ball”、“game”、“football”、“play”、“quarterback”的文本很可能是關於足球的——模型不知道這是足球，但它會知道這些詞往往一起出現 , 並且包含這些詞的文檔很可能是關於同一主題的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0  ['amp imf', 'amp imf loan', 'bank amp', 'bank amp imf', 'bank directors', 'bank directors says']\n",
      "Topic:1  ['gt good', 'lt gt good', 'aaniye', 'aaniye pudunga', 'aaniye pudunga venaam', 'athletic']\n",
      "Topic:2  ['boost secret', 'boost secret energy', 'cn', 'secret energy', '2gthr', '2gthr drinking']\n",
      "Topic:3  ['breath', 'curry', 'named', 'planet', 'love start', 'attraction']\n",
      "Topic:4  ['bajarangabali', 'bajarangabali maruti', 'bajarangabali maruti pavanaputra', 'dodda', 'dodda problum', 'dodda problum nalli']\n",
      "Topic:5  ['ur cell', '4wrd', '4wrd dear', '4wrd dear loving', 'abt events', 'abt events espe']\n",
      "Topic:6  ['police', 'afternoon wife', 'afternoon wife called', 'arrested murderer', 'arrested murderer immediately', 'called police']\n",
      "Topic:7  ['academic', 'department', 'academic department', 'academic department tell', 'academic secretary', 'academic secretary current']\n",
      "Topic:8  ['shahjahan', 'killed', 'mumtaz', '4th wife', '4th wife wifes', 'arises']\n",
      "Topic:9  ['1stone', '1stone sun', '1stone sun rose', 'amp sack', 'amp sack ful', 'amp tht']\n",
      "Topic:10  ['beg just', 'beg just listen', 'clear things', 'clear things easy', 'creativity', 'creativity stifled']\n",
      "Topic:11  ['laundry', 'make bed', 'time clean', 'angry left', 'angry left hit', 'bored wouldn']\n",
      "Topic:12  ['amp concern', 'amp concern prior', 'amp fuelled', 'amp fuelled love', 'amp pain', 'amp pain pls']\n",
      "Topic:13  ['waves', 'crab', 'footprints', 'asked frnd', 'asked frnd clearing', 'beautiful footprints']\n",
      "Topic:14  ['big files', 'big files download', 'come want', 'come want send', 'connection sucks', 'connection sucks remember']\n",
      "Topic:15  ['zyada kisi ko', 'zyada', 'zyada kisi', 'hai jo', 'hai jo ham', 'hai zindgi']\n",
      "Topic:16  ['able pay', 'able pay charge', 'ahead month', 'ahead month end', 'askin ahead', 'askin ahead month']\n",
      "Topic:17  ['1st 5wkg', '1st 5wkg days', '22 65', '22 65 61', '382', '382 ubi']\n",
      "Topic:18  ['ali', 'lyf', 'lyfu', 'meow', 'ali halla', 'ali halla ke']\n",
      "Topic:19  ['4get person', 'dat girl', 'dat girl margaret', 'girl margaret', 'girl margaret hello', 'girl yes']\n",
      "Topic:20  ['aid', 'aid usmle', 'aid usmle work', 'bam', 'bam aid', 'bam aid usmle']\n",
      "Topic:21  ['embarassed', 'best choice yes', 'buy say', 'buy say happened', 'choice yes', 'choice yes wanted']\n",
      "Topic:22  ['aid', 'aid usmle', 'aid usmle work', 'bam', 'bam aid', 'bam aid usmle']\n",
      "Topic:23  ['temp', 'children handle', 'children handle malaria', 'completely stop', 'days completely', 'days completely stop']\n",
      "Topic:24  ['arrange shipping', 'arrange shipping cut', 'bids online', 'bids online arrange', 'care rest', 'care rest wud']\n",
      "Topic:25  ['gary', 'sorry didn', 'babe sorry', 'babe sorry didn', 'bucks don', 'bucks don know']\n",
      "Topic:26  ['differ', 'charges', '9ja person', '9ja person sending', 'account bank remove', 'account details cos']\n",
      "Topic:27  ['differ', 'charges', '9ja person', '9ja person sending', 'account bank remove', 'account details cos']\n",
      "Topic:28  ['amplikater', 'amplikater fidalfication', 'amplikater fidalfication champlaxigating', 'atrocious', 'atrocious wotz', 'atrocious wotz ur']\n",
      "Topic:29  ['grl', 'agalla boy hogli', 'agalla boy necklace', 'boy gold', 'boy gold chain', 'boy hogli']\n",
      "Topic:30  ['blood blood', 'blood blood heart', 'blood heart', 'blood heart heart', 'friends including', 'friends including like']\n",
      "Topic:31  ['ur lover', '87077', '07123456789', '07123456789 87077', '07123456789 87077 yahoo', '87077 yahoo']\n",
      "Topic:32  ['ask especially', 'ask especially girls', 'choosing', 'choosing phone', 'choosing phone plan', 'citizen smart']\n",
      "Topic:33  ['sorry hope', '2nite tell', '2nite tell every1', 'ava', 'ava goodtime', 'ava goodtime oli']\n",
      "Topic:34  ['ear', 'need come home', 'break times', 'break times woul', 'come home need', 'curfew']\n",
      "Topic:35  ['18 30pp', '18 30pp txt', '1st 5free', '1st 5free 50', '30pp', '30pp txt']\n",
      "Topic:36  ['0870 chatlines', '0870 chatlines inclu', '250 want', '250 want 800', '3mobile', '3mobile 0870']\n",
      "Topic:37  ['grl', 'agalla boy hogli', 'agalla boy necklace', 'boy gold', 'boy gold chain', 'boy hogli']\n",
      "Topic:38  ['000pes', '000pes 48 tb', '000pes 48', '12 000pes', '12 000pes 48', '48 tb']\n",
      "Topic:39  ['09094100151', '09094100151 use', '09094100151 use ur', '10p min mob', '1er', '1er stop']\n",
      "Topic:40  ['1st ur', '1st ur lovely', '2nd ur', '2nd ur sms', '3rd ur', '3rd ur nature']\n",
      "Topic:41  ['0870 chatlines', '0870 chatlines inclu', '250 want', '250 want 800', '3mobile', '3mobile 0870']\n",
      "Topic:42  ['08712317606', '2rcv', 'cam moby', 'cam moby wanna', 'hlp', 'hlp 08712317606']\n",
      "Topic:43  ['150p wk', '150p wk club4', '25 free', '25 free credits', '2wt', '87070']\n",
      "Topic:44  ['affection', 'affection care', 'affection care luv', 'angry childish', 'angry childish true', 'angry wid']\n",
      "Topic:45  ['08701752560', '08701752560 450p', '08701752560 450p days', '100 filthy', '100 filthy films', '450p']\n",
      "Topic:46  ['113', '113 bray', '113 bray wicklow', 'address envelope', 'address envelope drinks', 'box 113']\n",
      "Topic:47  ['150p wk', '150p wk club4', '25 free', '25 free credits', '2wt', '87070']\n",
      "Topic:48  ['000pes', '000pes 48', '12 000pes', '12 000pes 48', '48 tb', '48 tb james']\n",
      "Topic:49  ['03 05', '03 05 prize', '00 easter prize', '05 prize', '05 prize transferred', '09041940223']\n",
      "Topic:50  ['attitude', 'attitude romantic', 'attitude romantic shy', 'attractive', 'attractive funny', 'attractive funny lt']\n",
      "Topic:51  ['09050280520', '09050280520 subscribe', '09050280520 subscribe 25p', '121 chat rooms', '25p pm', '25p pm dps']\n",
      "Topic:52  ['al salam', 'al salam wahleykkum', 'allah meet', 'allah meet rakhesh', 'evening sir al', 'fine inshah']\n",
      "Topic:53  ['noline rentl bx420', 'rentl bx420', 'rentl bx420 ip4', 'ur mates play', 'videochat wid ur', 'wid ur']\n",
      "Topic:54  ['01223585334 cum wan', '01223585334', '2c', '2c pics', '2c pics gettin', '2end']\n",
      "Topic:55  ['0quit', '0quit edrunk', '0quit edrunk sorry', '2nhite', '2nhite ros', '2nhite ros xxxxxxx']\n",
      "Topic:56  ['drop tear', 'drop tear dsn', 'dsn hav prayrs', 'dsn hav words', 'dsn lik', 'dsn lik stay']\n",
      "Topic:57  ['118p', '118p msg', '118p msg rcvd', '16 118p', '16 118p msg', '4217']\n",
      "Topic:58  ['87121', '10 ls1', '10 ls1 3aj', '1000 claim', '1000 claim txt', '18 50']\n",
      "Topic:59  ['bcz', 'bcz mis', 'bcz mis love', 'dont mis', 'ennal', 'ennal prabha']\n",
      "Topic:60  ['2u rply', '2u rply poly', '8007 poly', '8007 poly breathe1', 'breathe1', 'breathe1 titles']\n",
      "Topic:61  ['acc wen', 'acc wen lt', 'credited lt', 'credited lt gt', 'draw acc', 'draw acc wen']\n",
      "Topic:62  ['answer den', 'answer den manage', 'asked girl', 'asked girl tell', 'boy asked', 'boy asked girl']\n",
      "Topic:63  ['cost 150ppm', 'end march', 'need sort', 'norm', '15 cos', '15 cos st']\n",
      "Topic:64  ['anniversary', 'afternoon glorious', 'afternoon glorious anniversary', 'anniversary day', 'anniversary day sweet', 'coaxing']\n",
      "Topic:65  ['84199', 'box39822', 'box39822 w111wx', 'box39822 w111wx 50', 'colour flag', 'colour flag yer']\n",
      "Topic:66  ['body repairs', 'body repairs quite', 'followin', 'guess worried', 'guess worried know', 'guide ovulation']\n",
      "Topic:67  ['anniversary', 'afternoon glorious', 'afternoon glorious anniversary', 'anniversary day', 'anniversary day sweet', 'coaxing']\n",
      "Topic:68  ['87066', '2u 3wks', '2u 3wks pls', '3wks', '3wks pls', '3wks pls pls']\n",
      "Topic:69  ['contacts', 'try come', 'come till', 'come till goodnight', 'contacts remember', 'contacts remember venugopal']\n",
      "Topic:70  ['2u rply', '2u rply poly', '8007 poly', '8007 poly breathe1', 'breathe1', 'breathe1 titles']\n",
      "Topic:71  ['want nice', 'abi hw', 'abi hw posted', 'card abi', 'card abi hw', 'doc pls']\n",
      "Topic:72  ['bt quiet', 'bt quiet leaves', 'close fight', 'close fight wit', 'coz somtimes dis', 'dem coz']\n",
      "Topic:73  ['16 gbp1', '16 gbp1 50', '200 week', '200 week weekly', '3uz 16', '3uz 16 gbp1']\n",
      "Topic:74  ['82468', 'text wap', 'text wap 82468', 'wap 82468', 'bookmark', 'bookmark text']\n",
      "Topic:75  ['celeb', 'pic jordan', 'pocketbabe', 'pocketbabe uk', 'want nice', 'abi hw']\n",
      "Topic:76  ['askin i_', 'askin i_ leaving', 'cos mayb', 'cos mayb i_', 'cos meeting', 'cos meeting today']\n",
      "Topic:77  ['body repairs', 'body repairs quite', 'followin', 'guess worried', 'guess worried know', 'guide ovulation']\n",
      "Topic:78  ['amp promise', 'amp promise ll', 'bring ur', 'bring ur smile', 'gray', 'gray remembr']\n",
      "Topic:79  ['amp promise', 'amp promise ll', 'bring ur', 'bring ur smile', 'gray', 'gray remembr']\n"
     ]
    }
   ],
   "source": [
    "for index, component in enumerate(svd_tmp.components_):\n",
    "    zipped = zip(tok_cols2, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:6]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    print(\"Topic:\"+str(index)+\" \", top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Truncated SVD\n",
    "\n",
    "Try to use the same text for predictions from the newsgroups last time. Try to use the TSVD with a limited number of components and see if the accuracy can stay similar to what we got last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\"]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, shuffle=True, random_state=42, remove=remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorize and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (x,y): (857, 226373)   Test (x,y): (570, 226373)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and prep datasets\n",
    "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "X_train = news_tf.fit_transform(data_train.data)\n",
    "y_train = data_train.target\n",
    "X_test = news_tf.transform(data_test.data)\n",
    "y_test = data_test.target\n",
    "print(\"Train (x,y):\", X_train.shape, \"  Test (x,y):\", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11840\\1539219554.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnews_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"scale\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'svd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsvd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnews_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnews_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnews_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mnews_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \"\"\"\n\u001b[0;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    346\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    349\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    853\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Elsa\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    871\u001b[0m                     \u001b[1;34m\"Cannot center sparse matrices: pass `with_mean=False` \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m                     \u001b[1;34m\"instead. See docstring for motivation and alternatives.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives."
     ]
    }
   ],
   "source": [
    "# Create Models\n",
    "tsvd = TruncatedSVD(n_components=20)\n",
    "news_steps = [(\"scale\", StandardScaler()), ('svd', tsvd), ('m', RandomForestClassifier())]\n",
    "news_model = Pipeline(steps=news_steps)\n",
    "news_model.fit(X_train, y_train)\n",
    "news_model.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at the Topics\n",
    "\n",
    "We can also take a look at what the topics identified in the data are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akeems/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['ico', 'ico tek', 'ico tek com', 'vice ico', 'vice ico tek']\n",
      "Topic 1:  ['just', 'god', 'people', 'don', 'think']\n",
      "Topic 2:  ['cobb', '3rd debate', '3rd debate cobb', '3rd debate cobb alexia', 'champaign urbana']\n",
      "Topic 3:  ['mom', 'men', 'isc', 'isc rit', 'isc rit edu']\n",
      "Topic 4:  ['freewill', 'angels freewill', 'angels freewill god', 'angels freewill god tells', 'freewill god']\n",
      "Topic 5:  ['deletion', 'alt atheism', 'argument', 'alt', 'atheism']\n",
      "Topic 6:  ['deletion', 'ra', 'mcconkie', 'lds', 'mormon']\n",
      "Topic 7:  ['start', 'just', 'deletion', 'hand', 'account smile']\n",
      "Topic 8:  ['context', 'quote context', 'quotes context', 'jim', 'quote']\n",
      "Topic 9:  ['messenger', 'koresh', 'carried says', 'carried says character', 'carried says character messenger']\n",
      "Topic 10:  ['washed', 'washed blood', 'bull', 'blood', 'lamb']\n",
      "Topic 11:  ['said surrender', 'surrender', 'didn', 'rb', 'broadcast message']\n",
      "Topic 12:  ['rb', 'young', 'preying', 'preying young', 'altar boy']\n",
      "Topic 13:  ['god', 'jesus', 'bible', 'christ', 'ignorance strength']\n",
      "Topic 14:  ['petition', 'public schools', 'dm', 'schools', 'reading']\n",
      "Topic 15:  ['kent', 'cheers kent', 'cheers', 'order', 'jose']\n",
      "Topic 16:  ['dean kaflowitz', 'dean', 'kaflowitz', 'post', 'thread']\n",
      "Topic 17:  ['finished writing', 'finished writing sequel', 'finished writing sequel bible', 'koresh finished', 'koresh finished writing']\n",
      "Topic 18:  ['broken', 'promises broken', 'promises', 'just', 'jose']\n",
      "Topic 19:  ['just', 'finished writing', 'finished writing sequel', 'finished writing sequel bible', 'koresh finished']\n",
      "Topic 20:  ['objective', 'morality', 'values', 'jose', 'san jose']\n",
      "Topic 21:  ['cheat hillary', 'hillary', 'cheat', 'objective', 'morality']\n",
      "Topic 22:  ['just', 'cheers', 'cheers kent', 'objective', 'kent']\n",
      "Topic 23:  ['promises broken', 'broken', 'promises', 'cheat hillary', 'hillary']\n",
      "Topic 24:  ['believing god prepared', 'believing god prepared eternal', 'blashephemers', 'blashephemers hell', 'blashephemers hell believing']\n",
      "Topic 25:  ['atheists', 'cheers kent', 'cheers', 'atheism', 'kent']\n",
      "Topic 26:  ['god', 'exist', 'atheism', 'believe', 'does']\n",
      "Topic 27:  ['islam', 'islamic', 'argument', 'list', 'atheists']\n",
      "Topic 28:  ['mary', 'ridiculous', 'way thinking', 'does', 'joseph']\n",
      "Topic 29:  ['think', 'bible', 'don think', 'things', 'cheers kent']\n",
      "Topic 30:  ['don', 'atheism', 'animals', 'just', 'isn']\n",
      "Topic 31:  ['objective', 'values', 'john', 'read', 'paradise']\n",
      "Topic 32:  ['just', 'qur', 'true', 'verse', 'lunatic']\n",
      "Topic 33:  ['explain pertains', 'explain pertains position', 'explain pertains position statement', 'pertains position', 'pertains position statement']\n",
      "Topic 34:  ['sure wrong', 'sure', 'did claim', 'claim', 'claim objective']\n",
      "Topic 35:  ['jews', 'koresh', 'agree', 'malcolm', 'fbi']\n",
      "Topic 36:  ['think', 'don', 'post', 'paradise', 'salvation']\n",
      "Topic 37:  ['depression', 'sex', 'extra marital', 'extra marital sex', 'marital']\n",
      "Topic 38:  ['bit argument', 'just bit argument', 'quote just', 'quote just bit', 'quote just bit argument']\n",
      "Topic 39:  ['moral', 'fact', 'theory', 'bit', 'peace']\n",
      "Topic 40:  ['motto', 'ignorance strength', 'strength', 'think', 'little']\n",
      "Topic 41:  ['strength', 'ignorance strength', 'ignorance', 'god', 'sure wrong']\n",
      "Topic 42:  ['don', 'paradise', 'compuserve', 'compuserve com', 'mormons']\n",
      "Topic 43:  ['does', 'religion', 'science', 'theory', 'ignorance strength']\n",
      "Topic 44:  ['sold sign', 'wrong', 'sign', 'sold', 'sorry']\n",
      "Topic 45:  ['sure wrong', 'wrong', 'sure', 'think', 'freedom']\n",
      "Topic 46:  ['heard', 'lunacy', 'know', 'loans', 'sold sign']\n",
      "Topic 47:  ['did', 'does', 'com', 'compuserve', 'compuserve com']\n",
      "Topic 48:  ['don', 'heard', 'bit', 'death', 'don think']\n",
      "Topic 49:  ['theory', 'sure wrong', 'sure', 'imaginative', 'imaginative theory']\n"
     ]
    }
   ],
   "source": [
    "terms = news_tf.get_feature_names()\n",
    "topics = []\n",
    "for index, component in enumerate(tsvd.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Results\n",
    "\n",
    "Using LSA is a good way to condense our feature set that is often extremely large and extremely sparse, especially when we are dealing a large dataset, as is common with NLP. \n",
    "\n",
    "Simple applications in which this technique is used are documented clustering in text analysis, recommender systems, and information retrieval. More detailed use-cases of topic modeling are:\n",
    "<ul>\n",
    "<li> <b>Resume Summarization:</b> It can help recruiters to evaluate resumes by a quick glance. They can reduce effort in filtering pile of resume.\n",
    "<li> <b>Search Engine Optimization:</b> online articles, blogs, and documents can be tag easily by identifying the topics and associated keywords, which can improve optimize search results.\n",
    "<li> <b>Recommender System Optimization:</b> recommender systems act as an information filter and advisor according to the user profile and previous history. It can help us to discover unvisited relevant content based on past visits.\n",
    "<li> <b>Improving Customer Support:</b> Discovering relevant topics and associated keywords in customer complaints and feedback for examples product and service specifications, department, and branch details. Such information help company to directly rotated the complaint in respective department.\n",
    "<li> <b>Healthcare Industry:</b> topic modeling can help us to extract useful and valuable information from unstructured medical reports. This information can be used for patients treatment and medical science research purpose.\n",
    "</ul>\n",
    "\n",
    "In general, non-neural network approaches to NLP tend to be present in areas where we need to be able to process text quickly, without lots of processing. Spam filters are the classic example - we need to say yes or no, without spending ages to do so or burdening an email service with lots of processing. The examples above are similar - we are trying to draw a simple-ish conclusion. This is also somewhere that our old friend Bayes and his classifiers are most commonly seen - they are very fast at generating predictions once trained, so for something like emails, that's likely to be a good choice.\n",
    "\n",
    "An important concept from this example is the idea of condesing multiple features down into a smaller feature set while attempting to maintain the information in the original, that is something we'll revisit with Principal Component Analysis (PCA), a similar technique that is more generally applicable, later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA 結果\n",
    "\n",
    "使用 LSA 是壓縮通常非常大且非常稀疏的特徵集的好方法，尤其是當我們處理大型數據集時，這在 NLP 中很常見。\n",
    "\n",
    "使用此技術的簡單應用程序是文本分析、推薦系統和信息檢索中的記錄聚類。 主題建模的更詳細用例是：\n",
    "<ul>\n",
    "<li> <b>Resume Summarization：</b>它可以幫助招聘人員通過快速瀏覽來評估簡歷。 他們可以減少篩選簡歷的工作量。\n",
    "<li> <b>搜索引擎優化：</b>在線文章、博客和文檔可以通過識別主題和相關關鍵字輕鬆標記，從而改進優化搜索結果。\n",
    "<li> <b>推薦系統優化：</b>推薦系統根據用戶個人資料和以前的歷史充當信息過濾器和顧問。 它可以幫助我們根據過去的訪問發現未訪問過的相關內容。\n",
    "<li> <b>改善客戶支持：</b>在客戶投訴和反饋中發現相關主題和相關關鍵字，例如產品和服務規格、部門和分支機構詳細信息。 這些信息幫助公司直接將投訴轉移到各個部門。\n",
    "<li> <b>醫療保健行業</b>：主題建模可以幫助我們從非結構化醫療報告中提取有用且有價值的信息。 此信息可用於患者治療和醫學科學研究目的。\n",
    "</ul>\n",
    "\n",
    "一般來說，NLP 的非神經網絡方法往往出現在我們需要能夠快速處理文本而不需要大量處理的領域。 \n",
    "\n",
    "垃圾郵件過濾器就是一個典型的例子——我們需要說是或否，而不需要花費很長時間來做這件事或者給電子郵件服務增加大量處理的負擔。\n",
    "\n",
    "上面的例子很相似——我們試圖得出一個簡單的結論。 這也是我們的老朋友貝葉斯和他的分類器最常見的地方——\n",
    "\n",
    "一旦經過訓練，它們生成預測的速度非常快，所以對於像電子郵件這樣的東西，這可能是一個不錯的選擇。\n",
    "\n",
    "這個例子中的一個重要概念是將多個特徵壓縮成一個較小的特徵集，同時試圖保持原始信息，\n",
    "\n",
    "這是我們將使用主成分分析 (PCA) 重新審視的東西，一種類似的技術更 一般適用，稍後再說。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and Classification\n",
    "\n",
    "In addition to calculating things solely directly from our data, we can also use some external tools that can help create embeddings that are a little better (hopefully). This is also a neural network running behind the scenes to help us out. Word2Vec is an algorithm made by Google that can help process text and produce embeddings. Word2Vec looks for associations of words that occur with each other. This is an excellent illustrated description of Word2Vec: https://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "### Word2Vec in Process\n",
    "\n",
    "Word2Vec generates its embeddings by looking at words in a sentence, and the surrounding words in that same sentence. This differs quite a bit from the data that we've generated with the vectorization, as this model is better able to capture the strutucre of a sentence, beyond only looking at the individual words. We will use word2vec for a couple of different things:\n",
    "<ul>\n",
    "<li> Primarily, we'll use word2vec in a \"two model\" sequence to set us up to do classifications. The word2vec model will replace the count/tf-idf scores that we previously used for our feature set with embeddings that it calculates as the w2v model trains. The w2v model is \"learning\" how to represent words with numbers, in this case dimensions in a multidimensional space.\n",
    "    <ul>\n",
    "    <li> The w2v training is what creates the N-dimension measurements of each token, those then feed into our feature set for our modelling. \n",
    "    </ul>\n",
    "<li> After the word2vec model is created, we can do things like check the similarity of words. \n",
    "</ul>\n",
    "\n",
    "#### Gensim\n",
    "\n",
    "Gensim is a package that we can install that has an implementation of Word2Vec that we can use pretty easily. This part just downloads some of the stuff we'll need, like stopwords. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 和分類\n",
    "\n",
    "除了完全直接根據我們的數據進行計算之外，我們還可以使用一些外部工具來幫助創建更好一點的嵌入（希望如此）。 \n",
    "\n",
    "這也是一個在幕後運行的神經網絡來幫助我們。 Word2Vec 是谷歌開發的一種算法，可以幫助處理文本和生成嵌入。 \n",
    "\n",
    "Word2Vec 尋找彼此出現的單詞的關聯。 這是對 Word2Vec 的出色說明說明：https://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "### Word2Vec 正在處理中\n",
    "\n",
    "Word2Vec 通過查看句子中的單詞以及同一個句子中周圍的單詞來生成嵌入。 \n",
    "\n",
    "這與我們通過矢量化生成的數據有很大不同，因為該模型能夠更好地捕捉句子的結構，而不僅僅是查看單個單詞。 \n",
    "\n",
    "我們將使用 word2vec 來做一些不同的事情：\n",
    "<ul>\n",
    "<li> 首先，我們將在“雙模型”序列中使用 word2vec 來設置我們進行分類。 \n",
    "\n",
    "word2vec 模型將用它在 w2v 模型訓練時計算的嵌入替換我們之前用於特徵集的計數/tf-idf 分數。 \n",
    "\n",
    "w2v 模型正在“學習”如何用數字表示單詞，在這種情況下是多維空間中的維度。\n",
    "     <ul>\n",
    "     <li> w2v 訓練是為每個標記創建 N 維測量的原因，然後將這些測量輸入到我們的建模特徵集中。\n",
    "     </ul>\n",
    "<li> 創建 word2vec 模型後，我們可以做一些事情，比如檢查單詞的相似度。\n",
    "</ul>\n",
    "\n",
    "#### 基因模擬\n",
    "\n",
    "Gensim 是一個我們可以安裝的包，它有一個我們可以很容易使用的 Word2Vec 實現。 \n",
    "\n",
    "這部分只是下載一些我們需要的東西，比如停用詞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Elsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Elsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Elsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "for package in ['stopwords','punkt','wordnet']:\n",
    "    nltk.download(package) \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Since we are not using the vecorizer from sklearn, we need to provide our own tokenization. We can use the nltk based one from last time. We can also do any other types of processing here that we may want - stemming, customized stop words, etc... For this one I chopped out any 1 character tokens and added a regex filter to get rid of punctuation. \n",
    "\n",
    "### 分詞器\n",
    "\n",
    "由於我們沒有使用 sklearn 中的 vecorizer，因此我們需要提供自己的標記化。 我們可以使用上次的基於 nltk 的。 \n",
    "\n",
    "我們還可以在這裡進行我們可能想要的任何其他類型的處理——詞幹提取、自定義停用詞等……對於這個，我切掉了任何 1 個字符的標記，並添加了一個正則表達式過濾器來去除標點符號。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                tok = re.sub('\\W+','', tok) #Punctuation strip\n",
    "                tmp = self.lemmatizer.lemmatize(tok)\n",
    "                if len(tmp) >= 2:\n",
    "                    filtered_tok.append(tmp)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Clan Text - Tokenize and Lemmatize\n",
    "\n",
    "Prep some data. The \"second half\" of the dataframe is what we can use with the Word2Vec prediction models - we have cleaned up lists of tokens as well as translating the targets to 1 and 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, wkly, comp, win, FA, Cup, final,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, early, hor, already, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, nt, think, go, usf, life, around, though]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_text  target2  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, g...        0  \n",
       "1                        [Ok, lar, Joking, wif, oni]        0  \n",
       "2  [Free, entry, wkly, comp, win, FA, Cup, final,...        1  \n",
       "3               [dun, say, early, hor, already, say]        0  \n",
       "4    [Nah, nt, think, go, usf, life, around, though]        0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = lemmaTokenizer(stop_words)\n",
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: tok(x))\n",
    "df[\"target2\"] = pd.get_dummies(df[\"target\"], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word2Vec Ebmeddings\n",
    "\n",
    "Now comes the word2vec model - instead of taking our clean data and counting it to extract features, we can train our Word2Vec model with our cleaned up data and the output of that model is our set of features. This will have Word2Vec do its magic behind the scenes and perform the training. W2V works in one of two ways, which are roughly opposites of each other, when doing this training:\n",
    "<ul>\n",
    "<li> Continuous Bag of Words: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at words surrounding target to try to predict it. \n",
    "</ul>\n",
    "\n",
    "We'll revisit the details of some of this stuff later on when we look at neural networks, since W2V is a neural network algorithm, it will make more sense in context. \n",
    "\n",
    "<b>Note:</b> this training is not making a model that we are using to make predictions. This is training inside the W2V algorithm to generate representations of our tokens. \n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "The embeddings that we are generating are vectors that represent the words in our text. We can look at the embeddings for a word to see what they look like, but they aren't comprehensible to humans. Our count vectors or the td-idf calculations we made previously are also embeddings, those are just far more simple. Word2Vec will generate embeddings that attempt to group words that are similar together in multidimensional space. We can look at a simple example in 2D:\n",
    "\n",
    "![Similarity](images/similarity.png \"Similarity\")\n",
    "\n",
    "The values here aren't calculated, they are chosen arbitrarily, but each word is represented here in two dimensions - x and y. Words that are similar in meaning should be close to each other in the vector representation, such as \"King\" and \"Queen\". Words that are not similar should be far apart, such as \"King\" and \"Rutabaga\". The embeddings that word2vec will generate from our data as a result of the training below will aim to represent each word in 200 dimension space. We feed the word2vec model our tokens, and it will generate a N-dimension vector for each token. We can use comparisons in this N dimensional space to determine how similar two words are. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 創建 Word2Vec Ebmeddings\n",
    "\n",
    "現在出現了 word2vec 模型——我們可以用我們清理過的數據訓練我們的 Word2Vec 模型，而不是使用我們乾淨的數據併計算它來提取特徵，並且該模型的輸出是我們的特徵集。 \n",
    "\n",
    "這將使 Word2Vec 在幕後施展魔法並執行訓練。 在進行此訓練時，W2V 以兩種方式之一工作，這兩種方式大致相反：\n",
    "<ul>\n",
    "<li> 連續詞袋：查看每個目標詞周圍的窗口以嘗試預測周圍的詞。\n",
    "<li> Skip-Gram：查看目標周圍的詞以嘗試預測它。\n",
    "</ul>\n",
    "\n",
    "稍後當我們研究神經網絡時，我們將重新審視其中一些細節，因為 W2V 是一種神經網絡算法，它在上下文中會更有意義。\n",
    "\n",
    "<b>注意：</b> 此訓練並未製作我們用來進行預測的模型。 這是在 W2V 算法內部進行訓練，以生成我們的令牌表示。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 嵌入\n",
    "\n",
    "我們生成的嵌入是表示文本中單詞的向量。 我們可以查看單詞的嵌入以了解它們的外觀，但人類無法理解它們。 \n",
    "\n",
    "我們之前進行的計數向量或 td-idf 計算也是嵌入，只是簡單得多。 Word2Vec 將生成嵌入，試圖將多維空間中相似的單詞組合在一起。 \n",
    "\n",
    "這裡的值不是計算出來的，它們是任意選擇的，但是每個詞在這里以二維 - x 和 y 表示。 \n",
    "\n",
    "意思相近的詞在向量表示中應該靠得很近，比如“King”和“Queen”。 不相近的詞要相距遠，如“王”、“大頭菜”。 \n",
    "\n",
    "作為下面訓練的結果，word2vec 將從我們的數據生成的嵌入旨在表示 200 維空間中的每個單詞。 我們向 word2vec 模型提供我們的標記，它會為每個標記生成一個 N 維向量。 \n",
    "\n",
    "我們可以在這個 N 維空間中使用比較來確定兩個詞的相似程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import inspect\n",
    "  \n",
    "# use signature()\n",
    "print(inspect.signature(Word2Vec))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model Architecture\n",
    "\n",
    "We've mentioned that the word2vec model we are making is a neural network. Neural networks, as we'll see later, have an architecture, or basically a size and design. The details don't matter too much to us yet, but one thing that we can change when determing our model in word2vec is that architecture - we can choose between CBOW and Skip-Gram. These two options are roughly opposites of each other. The details of how they work and how they differ are neural network details, so we'll set those details aside for now.\n",
    "\n",
    "我們已經提到我們正在製作的 word2vec 模型是一個神經網絡。 正如我們稍後將看到的，神經網絡具有架構，或者基本上具有尺寸和設計。 \n",
    "\n",
    "細節對我們來說還不太重要，但是在確定 word2vec 模型時我們可以改變的一件事是架構——我們可以在 CBOW 和 Skip-Gram 之間進行選擇。 \n",
    "\n",
    "這兩個選項大致相反。 它們如何工作以及它們有何不同的細節是神經網絡的細節，所以我們暫時把這些細節放在一邊。\n",
    "\n",
    "<b>Continuous Bag of Words</b></br>\n",
    "![CBOW](images/cbow.webp \"CBOW\")\n",
    "\n",
    "<b>Skip-gram</b><br>\n",
    "![Skip-Gram](images/skip_gram.webp \"Skip-Gram\")\n",
    "\n",
    "These two models look like mirror images of each other, but what do they mean? Each does the same thing, though in a slightly different way. \n",
    "<ul>\n",
    "<li> CBOW: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at a word and tries to predict the surrounding words.\n",
    "</ul>\n",
    "\n",
    "For us, we can ignore the details of the differnece and think of the two options similarly to other options like regularization or entropy/gini. The way the internal neural network learns is different in the different architectures. \n",
    "\n",
    "#### Which to Use?\n",
    "\n",
    "For the most part, the real answer is our favorite one - test and choose the best. In general:\n",
    "<ul>\n",
    "<li> Skip Gram tends to work well with small amount of data and is found to represent rare words well.\n",
    "<li> CBOW is normally faster and has better representations for more frequent words.\n",
    "</ul>\n",
    "\n",
    "Parameters other than the ones we have listed here can be tweaked, but we'll somewhat ignore them for now, we're ok with the defaults. The \"sg\" parameter is the one that controls the architecture - 1 is skip-gram, 0 is CBOW."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這兩個模型看起來像是彼此的鏡像，但它們是什麼意思呢？ 每個人都做同樣的事情，儘管方式略有不同。\n",
    "<ul>\n",
    "<li> CBOW：查看每個目標詞周圍的窗口以嘗試預測周圍的詞。\n",
    "<li> Skip-Gram：查看一個詞並嘗試預測周圍的詞。\n",
    "</ul>\n",
    "\n",
    "對我們來說，我們可以忽略差異的細節，並以類似於正則化或熵/基尼等其他選項的方式來考慮這兩個選項。 \n",
    "\n",
    "內部神經網絡的學習方式在不同的架構中是不同的。\n",
    "\n",
    "#### 使用哪個？\n",
    "\n",
    "在大多數情況下，真正的答案是我們最喜歡的答案——測試並選擇最好的。 一般來說：\n",
    "<ul>\n",
    "<li> Skip Gram 往往適用於少量數據，並且被發現可以很好地表示稀有詞。\n",
    "<li> CBOW 通常更快並且對更頻繁的詞有更好的表示。\n",
    "</ul>\n",
    "\n",
    "我們在這裡列出的參數以外的參數可以調整，但我們暫時忽略它們，我們可以使用默認值。 \n",
    "\n",
    "“sg”參數是控制架構的參數——1 是 skip-gram，0 是 CBOW。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model = Word2Vec(df['clean_text'],min_count=3, vector_size=200, sg=1)\n",
    "#min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "#combination of word and its vector\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  \n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model\n",
    "\n",
    "Each word in the vocabulary now has a vector representing it - of size 200. We can make a dataframe and see each token in our text and its vector representation. This vector is the internal representation of each token that is generated by Word2Vec. This is how the algorithm calculates things like similarity...\n",
    "\n",
    "The word2vec result that we are printing out here is each word in our vocabulary and its vector representation - or all of its dimensions in the 200D space we created while the model was trained. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 模型\n",
    "\n",
    "詞彙表中的每個單詞現在都有一個表示它的向量——大小為 200。我們可以製作一個數據框並查看文本中的每個標記及其向量表示。 \n",
    "\n",
    "該向量是 Word2Vec 生成的每個標記的內部表示。 這就是算法如何計算相似度之類的東西......\n",
    "\n",
    "我們在這裡打印出的 word2vec 結果是我們詞彙表中的每個單詞及其向量表示 - 或者我們在訓練模型時創建的 200D 空間中的所有維度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>nt</th>\n",
       "      <th>get</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>ur</th>\n",
       "      <th>You</th>\n",
       "      <th>go</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>...</th>\n",
       "      <th>community</th>\n",
       "      <th>88039</th>\n",
       "      <th>SkilGme</th>\n",
       "      <th>meetin</th>\n",
       "      <th>ti</th>\n",
       "      <th>anywhere</th>\n",
       "      <th>opening</th>\n",
       "      <th>diff</th>\n",
       "      <th>living</th>\n",
       "      <th>boye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148444</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>0.067295</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>-0.022945</td>\n",
       "      <td>0.114367</td>\n",
       "      <td>0.085709</td>\n",
       "      <td>0.064109</td>\n",
       "      <td>0.063963</td>\n",
       "      <td>0.054185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040629</td>\n",
       "      <td>0.041977</td>\n",
       "      <td>0.037886</td>\n",
       "      <td>0.050819</td>\n",
       "      <td>0.048504</td>\n",
       "      <td>0.024622</td>\n",
       "      <td>0.029028</td>\n",
       "      <td>0.045932</td>\n",
       "      <td>0.037331</td>\n",
       "      <td>0.044939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.004675</td>\n",
       "      <td>-0.027695</td>\n",
       "      <td>-0.091960</td>\n",
       "      <td>-0.052563</td>\n",
       "      <td>-0.062537</td>\n",
       "      <td>-0.041445</td>\n",
       "      <td>0.008905</td>\n",
       "      <td>-0.032066</td>\n",
       "      <td>-0.029308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020359</td>\n",
       "      <td>-0.019174</td>\n",
       "      <td>-0.018625</td>\n",
       "      <td>-0.018485</td>\n",
       "      <td>-0.018153</td>\n",
       "      <td>-0.009105</td>\n",
       "      <td>-0.013288</td>\n",
       "      <td>-0.015207</td>\n",
       "      <td>-0.015266</td>\n",
       "      <td>-0.018489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.091548</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.053989</td>\n",
       "      <td>0.255615</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.097066</td>\n",
       "      <td>0.066605</td>\n",
       "      <td>0.028013</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.073416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.048582</td>\n",
       "      <td>0.038207</td>\n",
       "      <td>0.048829</td>\n",
       "      <td>0.037020</td>\n",
       "      <td>0.024018</td>\n",
       "      <td>0.024956</td>\n",
       "      <td>0.041867</td>\n",
       "      <td>0.039022</td>\n",
       "      <td>0.043074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.161302</td>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.107433</td>\n",
       "      <td>0.365333</td>\n",
       "      <td>0.379281</td>\n",
       "      <td>0.037254</td>\n",
       "      <td>0.053775</td>\n",
       "      <td>0.046690</td>\n",
       "      <td>0.063166</td>\n",
       "      <td>0.104828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>0.036919</td>\n",
       "      <td>0.030299</td>\n",
       "      <td>0.043386</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.017042</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.033105</td>\n",
       "      <td>0.027924</td>\n",
       "      <td>0.034405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.184827</td>\n",
       "      <td>0.132882</td>\n",
       "      <td>0.165953</td>\n",
       "      <td>0.323456</td>\n",
       "      <td>0.349884</td>\n",
       "      <td>0.183901</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>0.135485</td>\n",
       "      <td>0.144360</td>\n",
       "      <td>0.166430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079263</td>\n",
       "      <td>0.097863</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.102670</td>\n",
       "      <td>0.088945</td>\n",
       "      <td>0.051913</td>\n",
       "      <td>0.063504</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.079552</td>\n",
       "      <td>0.081643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.104215</td>\n",
       "      <td>0.063988</td>\n",
       "      <td>0.070005</td>\n",
       "      <td>-0.079753</td>\n",
       "      <td>-0.066267</td>\n",
       "      <td>0.085028</td>\n",
       "      <td>0.071899</td>\n",
       "      <td>0.122864</td>\n",
       "      <td>0.071898</td>\n",
       "      <td>0.053523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030069</td>\n",
       "      <td>0.038823</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.046448</td>\n",
       "      <td>0.038981</td>\n",
       "      <td>0.027305</td>\n",
       "      <td>0.028290</td>\n",
       "      <td>0.044103</td>\n",
       "      <td>0.033981</td>\n",
       "      <td>0.038333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.042627</td>\n",
       "      <td>0.084556</td>\n",
       "      <td>0.052244</td>\n",
       "      <td>-0.127450</td>\n",
       "      <td>-0.139348</td>\n",
       "      <td>0.067103</td>\n",
       "      <td>0.081465</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.068133</td>\n",
       "      <td>0.037919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026394</td>\n",
       "      <td>0.034827</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.031278</td>\n",
       "      <td>0.032232</td>\n",
       "      <td>0.022935</td>\n",
       "      <td>0.026195</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.033431</td>\n",
       "      <td>0.026697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.336169</td>\n",
       "      <td>-0.099395</td>\n",
       "      <td>-0.166457</td>\n",
       "      <td>-0.276381</td>\n",
       "      <td>-0.241740</td>\n",
       "      <td>-0.240602</td>\n",
       "      <td>-0.201684</td>\n",
       "      <td>-0.077809</td>\n",
       "      <td>-0.132518</td>\n",
       "      <td>-0.115541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087906</td>\n",
       "      <td>-0.114379</td>\n",
       "      <td>-0.087989</td>\n",
       "      <td>-0.112439</td>\n",
       "      <td>-0.106283</td>\n",
       "      <td>-0.065522</td>\n",
       "      <td>-0.073919</td>\n",
       "      <td>-0.108034</td>\n",
       "      <td>-0.094705</td>\n",
       "      <td>-0.103644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.172427</td>\n",
       "      <td>-0.054020</td>\n",
       "      <td>-0.033489</td>\n",
       "      <td>0.218018</td>\n",
       "      <td>0.189746</td>\n",
       "      <td>-0.105011</td>\n",
       "      <td>-0.104539</td>\n",
       "      <td>-0.047957</td>\n",
       "      <td>-0.056027</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036832</td>\n",
       "      <td>-0.044260</td>\n",
       "      <td>-0.028509</td>\n",
       "      <td>-0.044625</td>\n",
       "      <td>-0.046171</td>\n",
       "      <td>-0.025418</td>\n",
       "      <td>-0.025121</td>\n",
       "      <td>-0.041894</td>\n",
       "      <td>-0.037110</td>\n",
       "      <td>-0.039461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.071436</td>\n",
       "      <td>-0.095333</td>\n",
       "      <td>-0.041260</td>\n",
       "      <td>-0.086006</td>\n",
       "      <td>-0.053365</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>-0.005500</td>\n",
       "      <td>-0.061997</td>\n",
       "      <td>-0.065806</td>\n",
       "      <td>-0.056961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>-0.001575</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>-0.006205</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.007575</td>\n",
       "      <td>-0.001635</td>\n",
       "      <td>-0.002524</td>\n",
       "      <td>-0.003940</td>\n",
       "      <td>-0.009226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         call        nt       get        gt        lt        ur       You  \\\n",
       "0    0.148444  0.063375  0.067295  0.016624 -0.022945  0.114367  0.085709   \n",
       "1   -0.038233 -0.004675 -0.027695 -0.091960 -0.052563 -0.062537 -0.041445   \n",
       "2    0.091548  0.017172  0.053989  0.255615  0.270400  0.097066  0.066605   \n",
       "3    0.161302  0.074408  0.107433  0.365333  0.379281  0.037254  0.053775   \n",
       "4    0.184827  0.132882  0.165953  0.323456  0.349884  0.183901  0.156966   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.104215  0.063988  0.070005 -0.079753 -0.066267  0.085028  0.071899   \n",
       "196  0.042627  0.084556  0.052244 -0.127450 -0.139348  0.067103  0.081465   \n",
       "197 -0.336169 -0.099395 -0.166457 -0.276381 -0.241740 -0.240602 -0.201684   \n",
       "198 -0.172427 -0.054020 -0.033489  0.218018  0.189746 -0.105011 -0.104539   \n",
       "199  0.071436 -0.095333 -0.041260 -0.086006 -0.053365  0.003383 -0.005500   \n",
       "\n",
       "           go      know      like  ...  community     88039   SkilGme  \\\n",
       "0    0.064109  0.063963  0.054185  ...   0.040629  0.041977  0.037886   \n",
       "1    0.008905 -0.032066 -0.029308  ...  -0.020359 -0.019174 -0.018625   \n",
       "2    0.028013  0.042960  0.073416  ...   0.032520  0.048582  0.038207   \n",
       "3    0.046690  0.063166  0.104828  ...   0.028579  0.036919  0.030299   \n",
       "4    0.135485  0.144360  0.166430  ...   0.079263  0.097863  0.070170   \n",
       "..        ...       ...       ...  ...        ...       ...       ...   \n",
       "195  0.122864  0.071898  0.053523  ...   0.030069  0.038823  0.031893   \n",
       "196  0.088400  0.068133  0.037919  ...   0.026394  0.034827  0.022689   \n",
       "197 -0.077809 -0.132518 -0.115541  ...  -0.087906 -0.114379 -0.087989   \n",
       "198 -0.047957 -0.056027 -0.005327  ...  -0.036832 -0.044260 -0.028509   \n",
       "199 -0.061997 -0.065806 -0.056961  ...   0.000587 -0.001575  0.001124   \n",
       "\n",
       "       meetin        ti  anywhere   opening      diff    living      boye  \n",
       "0    0.050819  0.048504  0.024622  0.029028  0.045932  0.037331  0.044939  \n",
       "1   -0.018485 -0.018153 -0.009105 -0.013288 -0.015207 -0.015266 -0.018489  \n",
       "2    0.048829  0.037020  0.024018  0.024956  0.041867  0.039022  0.043074  \n",
       "3    0.043386  0.032707  0.017042  0.027508  0.033105  0.027924  0.034405  \n",
       "4    0.102670  0.088945  0.051913  0.063504  0.087912  0.079552  0.081643  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  0.046448  0.038981  0.027305  0.028290  0.044103  0.033981  0.038333  \n",
       "196  0.031278  0.032232  0.022935  0.026195  0.033230  0.033431  0.026697  \n",
       "197 -0.112439 -0.106283 -0.065522 -0.073919 -0.108034 -0.094705 -0.103644  \n",
       "198 -0.044625 -0.046171 -0.025418 -0.025121 -0.041894 -0.037110 -0.039461  \n",
       "199 -0.006205 -0.009349 -0.007575 -0.001635 -0.002524 -0.003940 -0.009226  \n",
       "\n",
       "[200 rows x 3150 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(w2v)\n",
    "vectors = model.wv\n",
    "tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Similarity\n",
    "\n",
    "One of the things that Word2Vec allows us to do is to look at the similarity of words. This similarity is calculated via the cosine distance of the vectors. Cosine similarity is a technique to calculate the distance between two vectors - smaller distance, more similar. \n",
    "\n",
    "![Cosine Similarity](images/cosine_sim.png \"Cosine Similarity\" )\n",
    "\n",
    "Once the vectors are derived by in the training process, these similarity calculations are pretty easy and quick. \n",
    "\n",
    "<b>Note:</b> the similarites here are calculated by the values derived from our trained model. So they are based on the relationships in our text. Word2Vec and other NLP packages also commonly have pretrained models that can be downloaded that are based on large amounts of text. Words may be represented very differently in those vs whatever we train here - the more data we have, the more consistent they'll be; the more \"unique\" our text is, the more different it will be. If we were, for example, working in a specific domain such as patent law, we could use a large amount of patent law text to train a model that would be more consistent with our domain. Or, perhaps more likely, we could use a pretrained model that has been created with massive amounts of training data. \n",
    "\n",
    "### Types of Similarity\n",
    "\n",
    "When looking at the similarity of different words, we can measure that similarity in a couple of ways - lexical and semantic, that we mentioned before. Here, the model is looking at semantic similarity, the \"meaning\" of each word, in the context of our text, is being compared and the most similar words are returned. Note that we can only calculate similarity here for words that we have in our vocabulary. This is one place where large language models like chat GPT have a massive advantage, their vocabulary is huge. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>注意：</b> 此處的相似度是根據我們經過訓練的模型得出的值計算得出的。 所以它們是基於我們文本中的關係。 \n",
    "\n",
    "Word2Vec 和其他 NLP 包通常還具有可以下載的基於大量文本的預訓練模型。 \n",
    "\n",
    "這些詞與我們在這裡訓練的任何詞的表示可能非常不同——我們擁有的數據越多，它們就越一致； 我們的文本越“獨特”，它就會越不同。 \n",
    "\n",
    "例如，如果我們在專利法等特定領域工作，我們可以使用大量專利法文本來訓練與我們的領域更一致的模型。 \n",
    "\n",
    "或者，更有可能的是，我們可以使用使用大量訓練數據創建的預訓練模型。\n",
    "\n",
    "### 相似度類型\n",
    "\n",
    "在查看不同單詞的相似性時，我們可以通過兩種方式來衡量這種相似性——我們之前提到的詞彙和語義。 \n",
    "\n",
    "在這裡，模型正在查看語義相似性，比較文本上下文中每個單詞的“含義”，並返回最相似的單詞。 \n",
    "\n",
    "請注意，我們只能在此處計算詞彙表中單詞的相似度。 這是像聊天 GPT 這樣的大型語言模型具有巨大優勢的地方，它們的詞彙量很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('want', 0.9961519837379456),\n",
       " ('ca', 0.9947837591171265),\n",
       " ('thing', 0.9945001602172852)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the most similar word to anything in our vocabulary \n",
    "vectors.most_similar(\"know\")[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do ok\n",
      "0.9721367\n"
     ]
    }
   ],
   "source": [
    "# We can also see how similar different words are. \n",
    "# I will grab two arbitrary words from the vocabulary and see how similar they are.\n",
    "# you could use anything in the vocabulary here, try some other ones!\n",
    "\n",
    "word_a = tmp.columns[30]\n",
    "word_b = tmp.columns[40]\n",
    "\n",
    "print(word_a, word_b)\n",
    "print(vectors.similarity(word_a, word_b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "We can take our actual data now and transform it through the Word2Vec model that we've made. This will generate our smaller feature set that we can build our models from, one of the things that the MeanEmbeddingVectorizer does is to collapse the data down to those 200 dimensions in the vector. Our dataset is spit out the other end, each row of text is now represented by a single vector of those 200 dimensions of our embedding values from the word2vec model (the columns).\n",
    "\n",
    "<b>Note:</b> if this is confusing, please ignore it, this is a bit of a tangent. The meanembeddingvectorizer thing is needed to \"flatten\" our data down from 200D to 1D for each token. This is because our models can only dal with data that is in that format (instances x features). We can't have a 200D vector for each token, we need to collapse it down to a single value. Later, when we look at neural networks, we'll see models with differnet architectures that can accomadate data that is multidimensional like this. That's one of the reasons that neural networks are so powerful, they can accomadate data that is multidimensional, so something like an image can be treated like an image, not just a bunch of pixels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＃＃＃ 作出預測\n",
    "\n",
    "我們現在可以獲取實際數據並通過我們製作的 Word2Vec 模型對其進行轉換。 \n",
    "\n",
    "這將生成我們可以從中構建模型的更小的特徵集，MeanEmbeddingVectorizer 所做的其中一件事是將數據折疊到向量中的 200 個維度。 \n",
    "\n",
    "我們的數據集從另一端吐出，每行文本現在由來自 word2vec 模型（列）的嵌入值的那 200 個維度的單個向量表示。\n",
    "\n",
    "<b>注意：</b>如果這讓您感到困惑，請忽略它，這有點離題。 需要 meanembeddingvectorizer 東西來將每個標記的數據從 200D“扁平化”到 1D。 \n",
    "\n",
    "這是因為我們的模型只能處理那種格式的數據（實例 x 特徵）。 我們不能為每個標記都有一個 200D 向量，我們需要將它折疊成一個值。 \n",
    "\n",
    "稍後，當我們研究神經網絡時，我們會看到具有不同網絡架構的模型可以容納像這樣的多維數據。 \n",
    "\n",
    "這就是神經網絡如此強大的原因之一，它們可以容納多維數據，所以像圖像這樣的東西可以像圖像一樣對待，而不僅僅是一堆像素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 200) (1393, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067330</td>\n",
       "      <td>-0.031478</td>\n",
       "      <td>0.070907</td>\n",
       "      <td>0.078295</td>\n",
       "      <td>0.158155</td>\n",
       "      <td>-0.123093</td>\n",
       "      <td>-0.087629</td>\n",
       "      <td>0.210541</td>\n",
       "      <td>-0.034585</td>\n",
       "      <td>0.125569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097020</td>\n",
       "      <td>-0.078663</td>\n",
       "      <td>-0.034641</td>\n",
       "      <td>-0.016192</td>\n",
       "      <td>0.135933</td>\n",
       "      <td>0.066504</td>\n",
       "      <td>0.046844</td>\n",
       "      <td>-0.170139</td>\n",
       "      <td>-0.054585</td>\n",
       "      <td>-0.018949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072015</td>\n",
       "      <td>-0.020641</td>\n",
       "      <td>0.058071</td>\n",
       "      <td>0.075310</td>\n",
       "      <td>0.146229</td>\n",
       "      <td>-0.117535</td>\n",
       "      <td>-0.086293</td>\n",
       "      <td>0.220376</td>\n",
       "      <td>-0.025747</td>\n",
       "      <td>0.114096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103871</td>\n",
       "      <td>-0.075003</td>\n",
       "      <td>-0.047092</td>\n",
       "      <td>-0.045035</td>\n",
       "      <td>0.123453</td>\n",
       "      <td>0.080726</td>\n",
       "      <td>0.061001</td>\n",
       "      <td>-0.152627</td>\n",
       "      <td>-0.070114</td>\n",
       "      <td>-0.023008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065776</td>\n",
       "      <td>-0.019488</td>\n",
       "      <td>0.055875</td>\n",
       "      <td>0.084187</td>\n",
       "      <td>0.151737</td>\n",
       "      <td>-0.101624</td>\n",
       "      <td>-0.081254</td>\n",
       "      <td>0.245679</td>\n",
       "      <td>-0.047375</td>\n",
       "      <td>0.118113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>-0.058960</td>\n",
       "      <td>-0.059484</td>\n",
       "      <td>-0.056558</td>\n",
       "      <td>0.123451</td>\n",
       "      <td>0.075468</td>\n",
       "      <td>0.065698</td>\n",
       "      <td>-0.138904</td>\n",
       "      <td>-0.055643</td>\n",
       "      <td>-0.050339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.073679</td>\n",
       "      <td>-0.031574</td>\n",
       "      <td>0.068011</td>\n",
       "      <td>0.062848</td>\n",
       "      <td>0.154091</td>\n",
       "      <td>-0.127947</td>\n",
       "      <td>-0.098683</td>\n",
       "      <td>0.212830</td>\n",
       "      <td>-0.026109</td>\n",
       "      <td>0.131231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109067</td>\n",
       "      <td>-0.088681</td>\n",
       "      <td>-0.032604</td>\n",
       "      <td>-0.037347</td>\n",
       "      <td>0.135932</td>\n",
       "      <td>0.076831</td>\n",
       "      <td>0.056990</td>\n",
       "      <td>-0.178684</td>\n",
       "      <td>-0.071122</td>\n",
       "      <td>-0.014774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.075539</td>\n",
       "      <td>-0.028012</td>\n",
       "      <td>0.061126</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>-0.120229</td>\n",
       "      <td>-0.097460</td>\n",
       "      <td>0.228929</td>\n",
       "      <td>-0.030222</td>\n",
       "      <td>0.122837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108183</td>\n",
       "      <td>-0.075713</td>\n",
       "      <td>-0.040561</td>\n",
       "      <td>-0.053916</td>\n",
       "      <td>0.129965</td>\n",
       "      <td>0.080973</td>\n",
       "      <td>0.066813</td>\n",
       "      <td>-0.163958</td>\n",
       "      <td>-0.069962</td>\n",
       "      <td>-0.027575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.078612</td>\n",
       "      <td>-0.029912</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>0.064049</td>\n",
       "      <td>0.158875</td>\n",
       "      <td>-0.131994</td>\n",
       "      <td>-0.095947</td>\n",
       "      <td>0.219392</td>\n",
       "      <td>-0.025709</td>\n",
       "      <td>0.128633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110893</td>\n",
       "      <td>-0.087051</td>\n",
       "      <td>-0.033683</td>\n",
       "      <td>-0.036254</td>\n",
       "      <td>0.135953</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.060425</td>\n",
       "      <td>-0.177465</td>\n",
       "      <td>-0.070211</td>\n",
       "      <td>-0.018766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.061854</td>\n",
       "      <td>-0.057025</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.038116</td>\n",
       "      <td>0.178205</td>\n",
       "      <td>-0.154648</td>\n",
       "      <td>-0.101647</td>\n",
       "      <td>0.206305</td>\n",
       "      <td>-0.019283</td>\n",
       "      <td>0.140963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122335</td>\n",
       "      <td>-0.096665</td>\n",
       "      <td>-0.033226</td>\n",
       "      <td>-0.027986</td>\n",
       "      <td>0.124033</td>\n",
       "      <td>0.049873</td>\n",
       "      <td>0.045605</td>\n",
       "      <td>-0.184484</td>\n",
       "      <td>-0.086276</td>\n",
       "      <td>0.008379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.081061</td>\n",
       "      <td>-0.040857</td>\n",
       "      <td>0.075242</td>\n",
       "      <td>0.060781</td>\n",
       "      <td>0.164685</td>\n",
       "      <td>-0.136626</td>\n",
       "      <td>-0.105341</td>\n",
       "      <td>0.215421</td>\n",
       "      <td>-0.025633</td>\n",
       "      <td>0.136803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113703</td>\n",
       "      <td>-0.093922</td>\n",
       "      <td>-0.027959</td>\n",
       "      <td>-0.027211</td>\n",
       "      <td>0.146967</td>\n",
       "      <td>0.075703</td>\n",
       "      <td>0.056903</td>\n",
       "      <td>-0.191305</td>\n",
       "      <td>-0.074009</td>\n",
       "      <td>-0.010794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>0.051889</td>\n",
       "      <td>-0.039262</td>\n",
       "      <td>0.109287</td>\n",
       "      <td>0.140094</td>\n",
       "      <td>0.194180</td>\n",
       "      <td>-0.113960</td>\n",
       "      <td>-0.048785</td>\n",
       "      <td>0.235141</td>\n",
       "      <td>-0.077077</td>\n",
       "      <td>0.138319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073464</td>\n",
       "      <td>-0.066842</td>\n",
       "      <td>-0.052576</td>\n",
       "      <td>0.055743</td>\n",
       "      <td>0.164489</td>\n",
       "      <td>0.035839</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>-0.182704</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>-0.038318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>0.103340</td>\n",
       "      <td>-0.055275</td>\n",
       "      <td>0.065998</td>\n",
       "      <td>0.068483</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>-0.173268</td>\n",
       "      <td>-0.119547</td>\n",
       "      <td>0.233281</td>\n",
       "      <td>-0.018184</td>\n",
       "      <td>0.165296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133199</td>\n",
       "      <td>-0.120234</td>\n",
       "      <td>-0.038438</td>\n",
       "      <td>-0.048985</td>\n",
       "      <td>0.166338</td>\n",
       "      <td>0.089687</td>\n",
       "      <td>0.055262</td>\n",
       "      <td>-0.265692</td>\n",
       "      <td>-0.083953</td>\n",
       "      <td>-0.004604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.067330 -0.031478  0.070907  0.078295  0.158155 -0.123093 -0.087629   \n",
       "1     0.072015 -0.020641  0.058071  0.075310  0.146229 -0.117535 -0.086293   \n",
       "2     0.065776 -0.019488  0.055875  0.084187  0.151737 -0.101624 -0.081254   \n",
       "3     0.073679 -0.031574  0.068011  0.062848  0.154091 -0.127947 -0.098683   \n",
       "4     0.075539 -0.028012  0.061126  0.063572  0.151786 -0.120229 -0.097460   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4174  0.078612 -0.029912  0.068667  0.064049  0.158875 -0.131994 -0.095947   \n",
       "4175  0.061854 -0.057025  0.072266  0.038116  0.178205 -0.154648 -0.101647   \n",
       "4176  0.081061 -0.040857  0.075242  0.060781  0.164685 -0.136626 -0.105341   \n",
       "4177  0.051889 -0.039262  0.109287  0.140094  0.194180 -0.113960 -0.048785   \n",
       "4178  0.103340 -0.055275  0.065998  0.068483  0.175227 -0.173268 -0.119547   \n",
       "\n",
       "           7         8         9    ...       190       191       192  \\\n",
       "0     0.210541 -0.034585  0.125569  ...  0.097020 -0.078663 -0.034641   \n",
       "1     0.220376 -0.025747  0.114096  ...  0.103871 -0.075003 -0.047092   \n",
       "2     0.245679 -0.047375  0.118113  ...  0.098110 -0.058960 -0.059484   \n",
       "3     0.212830 -0.026109  0.131231  ...  0.109067 -0.088681 -0.032604   \n",
       "4     0.228929 -0.030222  0.122837  ...  0.108183 -0.075713 -0.040561   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4174  0.219392 -0.025709  0.128633  ...  0.110893 -0.087051 -0.033683   \n",
       "4175  0.206305 -0.019283  0.140963  ...  0.122335 -0.096665 -0.033226   \n",
       "4176  0.215421 -0.025633  0.136803  ...  0.113703 -0.093922 -0.027959   \n",
       "4177  0.235141 -0.077077  0.138319  ...  0.073464 -0.066842 -0.052576   \n",
       "4178  0.233281 -0.018184  0.165296  ...  0.133199 -0.120234 -0.038438   \n",
       "\n",
       "           193       194       195       196       197       198       199  \n",
       "0    -0.016192  0.135933  0.066504  0.046844 -0.170139 -0.054585 -0.018949  \n",
       "1    -0.045035  0.123453  0.080726  0.061001 -0.152627 -0.070114 -0.023008  \n",
       "2    -0.056558  0.123451  0.075468  0.065698 -0.138904 -0.055643 -0.050339  \n",
       "3    -0.037347  0.135932  0.076831  0.056990 -0.178684 -0.071122 -0.014774  \n",
       "4    -0.053916  0.129965  0.080973  0.066813 -0.163958 -0.069962 -0.027575  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4174 -0.036254  0.135953  0.073171  0.060425 -0.177465 -0.070211 -0.018766  \n",
       "4175 -0.027986  0.124033  0.049873  0.045605 -0.184484 -0.086276  0.008379  \n",
       "4176 -0.027211  0.146967  0.075703  0.056903 -0.191305 -0.074009 -0.010794  \n",
       "4177  0.055743  0.164489  0.035839  0.014546 -0.182704 -0.000544 -0.038318  \n",
       "4178 -0.048985  0.166338  0.089687  0.055262 -0.265692 -0.083953 -0.004604  \n",
       "\n",
       "[4179 rows x 200 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_text\"],df[\"target2\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v = modelw.transform(X_train)\n",
    "X_test_vectors_w2v = modelw.transform(X_test)\n",
    "print(X_train_vectors_w2v.shape, X_test_vectors_w2v.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model\n",
    "\n",
    "We now have a pretty normal dataset and can use the new data to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9839124432884692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1194\n",
      "           1       0.93      0.86      0.90       199\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.95      0.93      0.94      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "lr_w2v = RandomForestClassifier()\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Word2Vec\n",
    "\n",
    "Use the newsgroup data and Word2Vec to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datsets and Tokenize\n",
    "tok = lemmaTokenizer(stop_words)\n",
    "X_w2v_news_train = [tok(x) for x in data_train.data]\n",
    "X_w2v_news_test = [tok(x) for x in data_test.data]\n",
    "\n",
    "y_train_news = data_train.target\n",
    "y_test_news = data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Benedikt',\n",
       " 'Rosenau',\n",
       " 'writes',\n",
       " 'great',\n",
       " 'authority',\n",
       " 'Contradictory',\n",
       " 'property',\n",
       " 'language',\n",
       " 'If',\n",
       " 'correct',\n",
       " 'THINGS',\n",
       " 'DEFINED',\n",
       " 'BY',\n",
       " 'CONTRADICTORY',\n",
       " 'LANGUAGE',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'EXIST',\n",
       " 'object',\n",
       " 'definition',\n",
       " 'reality',\n",
       " 'If',\n",
       " 'amend',\n",
       " 'THINGS',\n",
       " 'DESCRIBED',\n",
       " 'BY',\n",
       " 'CONTRADICTORY',\n",
       " 'LANGUAGE',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'EXIST',\n",
       " 've',\n",
       " 'come',\n",
       " 'something',\n",
       " 'plainly',\n",
       " 'false',\n",
       " 'Failures',\n",
       " 'description',\n",
       " 'merely',\n",
       " 'failure',\n",
       " 'description',\n",
       " 'objectivist',\n",
       " 'remember']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview\n",
    "X_w2v_news_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model_news = Word2Vec(X_w2v_news_train, min_count=1, vector_size=200)\n",
    "w2v_news = dict(zip(model_news.wv.index_to_key, model_news.wv.vectors)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_news_w = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_news = model_news_w.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_news = model_news_w.transform(X_w2v_news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70       319\n",
      "           1       0.44      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.50      0.50      0.40       570\n",
      "weighted avg       0.51      0.55      0.44       570\n",
      "\n",
      "Confusion Matrix:\n",
      " [[300  19]\n",
      " [236  15]]\n",
      "AUC: 0.5253343990807928\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf = SVC(probability=True)\n",
    "news_clf.fit(X_train_vectors_w2v_news, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news = news_clf.predict(X_val_vectors_w2v_news)\n",
    "y_prob_news = news_clf.predict_proba(X_val_vectors_w2v_news)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(y_test_news, y_predict_news))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_news, y_prob_news)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec Models\n",
    "\n",
    "When we do the intial training of the word2vec model (not when we are making final predictions), we are using our corpus to generate the space and the embeddigns for all of our tokens. We can also download a pretrained model, that already has the N-dimensional space defined (when it was trained on some different data), and use that to generate our embeddings. This is a common practice, and can be a good way to get started. Gensim has several models that have been trained on varying amounts of data, they are listed here: https://github.com/RaRe-Technologies/gensim-data along with several other datasets that we could use to train a model. \n",
    "\n",
    "The differences with using this pretrained model (or an existing corpus below) are:\n",
    "<ul>\n",
    "<li> Above, when training word2vec with our data, we used our corpus to generate the space in which the tokens are placed, then calculate those embeddings for each token. \n",
    "<li> With a pretrained model, we are using the space that was generated by the model that was trained on some other data, then placing our tokens in that space. \n",
    "</ul>\n",
    "\n",
    "So if we are using some text from wikipedia (like the second example), the space in which embeddings are made is defined by the text in wikipedia. So the \"closeness\" in meaning of words is based on what is in that corpus. We then take our tokens and calculate their embeddings in that space. The big advantage to this is someone else can train a model on lots of data, which hopefully generates a better understanding of the relationships between words, and we can then just score our words on those scales. This approach is common in large models, like text processing or image recognition, where the training load can be too large for \"regular folk\". We can also take these trained models and \"customize\" them to our data, we'll look at that with image recognition at the end of the semester. \n",
    "\n",
    "#### Use a Twitter Trained Model\n",
    "\n",
    "We can try using a pretrained model that was trained on Twitter data. This model has been pretrained, so it already knows how to represent words, we will then feed it all of our tokens, and it will generate the embeddings for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預訓練的 Word2Vec 模型\n",
    "\n",
    "當我們對 word2vec 模型進行初始訓練時（而不是在我們進行最終預測時），我們正在使用我們的語料庫為我們所有的標記生成空間和嵌入。 \n",
    "\n",
    "我們還可以下載一個預訓練模型，它已經定義了 N 維空間（當它在一些不同的數據上訓練時），並使用它來生成我們的嵌入。 \n",
    "\n",
    "這是一種常見的做法，也是一個很好的入門方式。 Gensim 有幾個模型已經過不同數量的數據訓練，\n",
    "\n",
    "\n",
    "使用這個預訓練模型（或下面現有的語料庫）的區別是：\n",
    "<ul>\n",
    "<li> 上面，當用我們的數據訓練 word2vec 時，我們使用我們的語料庫生成放置標記的空間，然後為每個標記計算這些嵌入。\n",
    "<li> 對於預訓練模型，我們正在使用由模型生成的空間，該模型是在其他一些數據上訓練的，然後將我們的標記放在該空間中。\n",
    "</ul>\n",
    "\n",
    "因此，如果我們使用維基百科中的一些文本（如第二個示例），則嵌入的空間由維基百科中的文本定義。 \n",
    "\n",
    "因此，詞義的“接近度”基於該語料庫中的內容。 然後我們獲取我們的標記併計算它們在該空間中的嵌入。 \n",
    "\n",
    "這樣做的一大優勢是其他人可以在大量數據上訓練模型，這有望更好地理解單詞之間的關係，然後我們可以在這些尺度上對單詞進行評分。 \n",
    "\n",
    "這種方法在文本處理或圖像識別等大型模型中很常見，對於“普通人”來說，這些模型的訓練負荷可能太大。 \n",
    "\n",
    "我們還可以採用這些經過訓練的模型並將它們“定制”到我們的數據中，我們將在學期結束時通過圖像識別來研究它。\n",
    "\n",
    "#### 使用 Twitter 訓練模型\n",
    "\n",
    "我們可以嘗試使用在 Twitter 數據上訓練過的預訓練模型。 這個模型已經過預訓練，所以它已經知道如何表示單詞，然後我們將所有的標記輸入它，它會為我們生成嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 0.9448828101158142),\n",
       " ('baby', 0.9425430297851562),\n",
       " ('dream', 0.9267040491104126),\n",
       " ('miss', 0.9246909022331238),\n",
       " ('much', 0.9215252995491028),\n",
       " ('see', 0.919786810874939),\n",
       " ('happy', 0.9176183938980103),\n",
       " ('beautiful', 0.9173233509063721),\n",
       " ('smile', 0.9138967394828796),\n",
       " ('loves', 0.9123677611351013)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downlaod the model and do a little test\n",
    "\n",
    "import gensim.downloader as api\n",
    "model_twit = api.load(\"glove-twitter-25\")\n",
    "model_twit.most_similar(\"love\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Embeddings\n",
    "\n",
    "The model exists, so we will use it to transform our tokens into numerical representations. Then we can go use those to make classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.58      0.71       517\n",
      "           1       0.13      0.60      0.21        53\n",
      "\n",
      "    accuracy                           0.58       570\n",
      "   macro avg       0.53      0.59      0.46       570\n",
      "weighted avg       0.86      0.58      0.67       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_twit = dict(zip(model_twit.index_to_key, model_twit.vectors))\n",
    "model_twit_emb = MeanEmbeddingVectorizer(w2v_twit)\n",
    "\n",
    "X_train_twit = model_twit_emb.transform(X_w2v_news_train)\n",
    "X_test_twit = model_twit_emb.transform(X_w2v_news_test)\n",
    "\n",
    "# Make predictions\n",
    "twit_clf = SVC(probability=True)\n",
    "twit_clf.fit(X_train_twit, y_train_news)  #model\n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict_twit = twit_clf.predict(X_test_twit)\n",
    "\n",
    "print(classification_report(y_predict_twit, y_test_news))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premade Corpus\n",
    "\n",
    "We can also train a model directly from a preexisting corpus, then generate our embeddings from that model. \n",
    "\n",
    "The \"text8\" corpus is a small corpus of text that is included with gensim. It is a small subset of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.load('text8')\n",
    "model_corp = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 200)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corp = dict(zip(model_corp.wv.index_to_key, model_corp.wv.vectors)) \n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_corp_emb = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_corp = model_corp_emb.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_corp = model_corp_emb.transform(X_w2v_news_test)\n",
    "X_train_vectors_w2v_corp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70       319\n",
      "           1       0.44      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.50      0.50      0.40       570\n",
      "weighted avg       0.51      0.55      0.44       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf_corp = SVC(probability=True)\n",
    "news_clf_corp.fit(X_train_vectors_w2v_corp, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news_corp = news_clf.predict(X_val_vectors_w2v_corp)\n",
    "y_prob_news_corp = news_clf.predict_proba(X_val_vectors_w2v_corp)[:,1]\n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news_corp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Me!\n",
    "\n",
    "As we see with things like chatGPT and the assortment of voice assistants, NLP is currently exploding in both capability and prevelence. Those other models are based on these concepts, but there are a few key differences that help those tools be more powerful:\n",
    "<ul>\n",
    "<li> They are trained on much larger datasets. Very, very, very large datasets. In NLP specifically, this helps because it can help address the problem with us having so many words, many of which aren't used super often - i.e. the fact that there are a lot of words that don't occur together in the same sentence. If the training data is massive (e.g. \"the internet\"), we massively reduce the impact of this problem, as we see each word many times. \n",
    "<li> The use of neural networks, in particular recurrant neural networks (RNNs) that are able to deal with data as a sequence, and \"remember\" other parts of a sequence of words. This helps these models understand the context of a sentence, and the relationships between words.\n",
    "    <ul>\n",
    "    <li> Of note with neural networks, especially those using massive training data sets, is that the first layers of the model can perform equivalent data prep work that we've done here. So the model is more able to deal with data in its raw form, and doesn't need to be preprocessed as much separately, in advance. \n",
    "    </ul>\n",
    "<li> Manual intervention is used, humans provide examples of convesation, define labels, and evaluate the quality of the model's work. You may have heard news of Kenyans being paid low wages to label data for these models.\n",
    "<li> Other model types are used to help, such as reinforcement learning. Responses that are good are rewarded, and those that are bad are punished. This helps the model learn what is good and what is bad. This is particularly useful for generative models, such as chatGPT.\n",
    "</ul>\n",
    "\n",
    "As noted, this stuff is actively being developed right now, and the more advanced the tool, the more likely we are to see innovation or specific interventions to correct issues. The foundations we have looked at here are the building blocks of that work. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP 和我！\n",
    "\n",
    "正如我們在 chatGPT 和各種語音助手中看到的那樣，NLP 目前在能力和流行度上都呈爆炸式增長。 \n",
    "\n",
    "那些其他模型基於這些概念，但有一些關鍵差異可以幫助這些工具變得更強大：\n",
    "<ul>\n",
    "<li> 他們在更大的數據集上接受訓練。 非常、非常、非常大的數據集。 \n",
    "\n",
    "特別是在 NLP 中，這很有幫助，因為它可以幫助解決我們有很多單詞的問題，其中許多單詞並不經常使用——即事實上有很多單詞不會同時出現在同一個句子中 . \n",
    "\n",
    "如果訓練數據很大（例如“互聯網”），我們會大大減少這個問題的影響，因為我們會多次看到每個單詞。\n",
    "\n",
    "<li> 使用神經網絡，特別是循環神經網絡 (RNN)，它能夠將數據作為序列處理，並“記住”單詞序列的其他部分。 \n",
    "\n",
    "這有助於這些模型理解句子的上下文以及單詞之間的關係。\n",
    "     <ul>\n",
    "     <li> 值得注意的是神經網絡，尤其是那些使用大量訓練數據集的神經網絡，模型的第一層可以執行我們在此處完成的等效數據準備工作。 \n",
    "     \n",
    "     因此，該模型更能夠處理原始形式的數據，並且不需要預先進行盡可能多的單獨預處理。\n",
    "     </ul>\n",
    "<li> 使用人工干預，人類提供對話示例、定義標籤並評估模型工作的質量。 \n",
    "\n",
    "您可能聽說過肯尼亞人為這些模型標記數據而獲得低工資的消息。\n",
    "\n",
    "<li> 其他模型類型用於提供幫助，例如強化學習。 好的反應會得到獎勵，不好的反應會受到懲罰。 \n",
    "\n",
    "這有助於模型了解什麼是好的，什麼是壞的。 這對於生成模型特別有用，例如 chatGPT。\n",
    "</ul>\n",
    "\n",
    "如前所述，這些東西現在正在積極開發中，工具越先進，我們就越有可能看到創新或特定乾預措施來糾正問題。 \n",
    "\n",
    "我們在這裡看到的基礎是這項工作的基石。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40846f95e88ae24f681f7d79d7396bca459ce37b2ecada686cfbc3bbe9daaf0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
