{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist, cifar10\n",
    "from keras.layers import Dense, Flatten, Normalization, Dropout, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomZoom, BatchNormalization, Activation, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import utils\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import PIL\n",
    "if IN_COLAB:\n",
    "    !pip install --ignore-installed Pillow==9.0.0\n",
    "    !pip install -U git+https://github.com/keisen/tf-keras-vis.git@4a90becb02ed3d44825300fcb807dd58157787ba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "### Feature Extraction and Classification\n",
    "\n",
    "One of the key concepts needed with transfer learning is the separating of the feature extraction from the convolutional layers and the classification done in the fully connected layers.\n",
    "<ul>\n",
    "<li> The convolutional layer finds features in the image. I.e. the output of the end of the convolutional layers is a set of image-y features. \n",
    "<li> The fully connected layers take those features and classify the thing. \n",
    "</ul>\n",
    "\n",
    "The idea behind this is that we allow someone (like Google) to train their fancy network on a bunch of fast computers, using millions and millions of images. These classifiers get very good at extracting features from objects. \n",
    "\n",
    "When using these models we take those convolutional layers and slap on our own classifier at the end, so the pretrained convolutional layers extract a bunch of features with their massive amount of training, then we use those features to predict our data!\n",
    "\n",
    "### Tensorboard Up-Front\n",
    "\n",
    "we'll also launch the tensorboard prior to doing any training. Pay attention to the log locations in each callback, we can nest the logs in folders, then use the names and tensorboard's regex search to monitor each run as it progresses. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遷移學習\n",
    "\n",
    "### 特徵提取和分類\n",
    "\n",
    "遷移學習所需的關鍵概念之一是將特徵提取與卷積層和在全連接層中完成的分類分開。\n",
    "<ul>\n",
    "<li> 卷積層在圖像中尋找特徵。 IE。 卷積層末端的輸出是一組 image-y 特徵。\n",
    "<li> 全連接層採用這些特徵並對事物進行分類。\n",
    "</ul>\n",
    "\n",
    "這背後的想法是，我們允許某人（比如穀歌）在一堆快速計算機上使用數百萬張圖像來訓練他們的奇特網絡。 這些分類器非常擅長從對像中提取特徵。\n",
    "\n",
    "當使用這些模型時，我們在最後使用這些卷積層和我們自己的分類器，因此預訓練的捲積層通過大量訓練提取了一堆特徵，然後我們使用這些特徵來預測我們的數據！\n",
    "\n",
    "### Tensorboard 前期\n",
    "\n",
    "我們還將在進行任何訓練之前啟動 tensorboard。 注意每個回調中的日誌位置，我們可以將日誌嵌套在文件夾中，然後使用名稱和 tensorboard 的正則表達式搜索來監控每次運行的進展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f9df165f227355f4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f9df165f227355f4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model\n",
    "\n",
    "There are several models that are pretrained and available to us to use. VGG16 is one developed to do image recognition, the name stands for \"Visual Geometry Group\" - a group of researchers at the University of Oxford who developed it, and ‘16’ implies that this architecture has 16 layers. The model got ~93% on the ImageNet test that we mentioned a couple of weeks ago. \n",
    "\n",
    "![VGG16](images/vgg16.png \"VGG16\" )\n",
    "\n",
    "#### Slide Convolutional Layers from Classifier\n",
    "\n",
    "When downloading the model we specifiy that we don't want the top - that's the classification part. When we remove the top we also allow the model to adapt to the shape of our images, so we specify the input size as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "Our VGG 16 model comes with a preprocessing function to prepare the data in a way it is happy with. For this model the color encoding that it was trained on is different, so we should prepare the data properly to get good results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228813984/228813984 [==============================] - 4s 0us/step\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n",
      "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                                   fname='flower_photos',\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "img_depth = 3\n",
    "\n",
    "train_ds_orig = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds_orig = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds_orig.class_names\n",
    "print(class_names)\n",
    "\n",
    "def preprocess(images, labels):\n",
    "  return tf.keras.applications.vgg16.preprocess_input(images), labels\n",
    "\n",
    "train_ds = train_ds_orig.map(preprocess)\n",
    "val_ds = val_ds_orig.map(preprocess)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Some Images for Tensorboard\n",
    "\n",
    "We'll record some images, both pre and post processing. The VGG model wants images to use a different representation than RGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.create_file_writer(\"logs/VGG/train_data\")\n",
    "train_ds_iterator = train_ds_orig.as_numpy_iterator()\n",
    "train_proc_iterator = train_ds.as_numpy_iterator()\n",
    "samp_batch  = train_ds_iterator.next()\n",
    "proc_batch = train_proc_iterator.next()\n",
    "\n",
    "with file_writer.as_default():\n",
    "    images = np.reshape(samp_batch[0].astype(\"uint8\"), (-1, img_height, img_width, img_depth))\n",
    "    procImages = np.reshape(proc_batch[0].astype(\"uint8\"), (-1, img_height, img_width, img_depth))\n",
    "    #images = samp_batch\n",
    "    tf.summary.image(\"32 Original Images\", images, max_outputs=32, step=0)\n",
    "    tf.summary.image(\"32 Processed Images\", procImages, max_outputs=32, step=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add on New Classifier\n",
    "\n",
    "If we look at the previous summary of the model we can see that the last layer we have is a MaxPool layer. When making our own CNN this is the last layer before we add in the \"normal\" stuff for making predictions, this is the same. We need to flatten the data, then use dense layers and an output layer to classify the predictions. \n",
    "\n",
    "We end up with the pretrained parts finding features in images, and the custom part classifying images based on those features. If we think back to the concept of a convolutional network, the convolutional layers do the true heavy lifting in allowing us to do things like classify images, they take in the raw images and transform it into a set of features contained in that image. This ability to turn images into predictive features is the key - important parts of images like edges, corners, contrast, etc... are generic, and our borrowed model is excellent at finding these features in images. Our predicitons are unique, so we tweak the training of our model to make predictions for our data, into our classes - all based on the features that the borrowed model found! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 添加新分類器\n",
    "\n",
    "如果我們查看模型的先前摘要，我們可以看到我們擁有的最後一層是 MaxPool 層。 在製作我們自己的 CNN 時，這是我們添加“正常”內容進行預測之前的最後一層，這是相同的。 \n",
    "\n",
    "我們需要展平數據，然後使用密集層和輸出層對預測進行分類。\n",
    "\n",
    "我們最終得到預訓練部分在圖像中尋找特徵，自定義部分根據這些特徵對圖像進行分類。 \n",
    "\n",
    "如果我們回想一下卷積網絡的概念，卷積層在允許我們做諸如分類圖像之類的事情方面做了真正的繁重工作，它們接收原始圖像並將其轉換為該圖像中包含的一組特徵。 \n",
    "\n",
    "這種將圖像轉化為預測特徵的能力是關鍵——圖像的重要部分，如邊緣、角、對比度等……是通用的，我們藉用的模型非常擅長在圖像中找到這些特徵。 \n",
    "\n",
    "我們的預測是獨一無二的，因此我們調整模型的訓練以對我們的數據進行預測，進入我們的類別——所有這些都基於借用模型發現的特徵！"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Model\n",
    "\n",
    "We take the model without the top, set the input image size, and then add our own classifier. Loading the model is simple, there are just a few things to specify:\n",
    "<ul>\n",
    "<li> weights=\"imagenet\" - tells the model to use the weights from its imagenet training. This is what brings the \"smarts\", so we want it. \n",
    "<li> include_top=False - tells the model to not bring over the classifier bits that we wnat to replace. \n",
    "<li> input_shape - the model is trained on specific data sizes (224x224x3). We can repurpose it by changing the input size. \n",
    "</ul>\n",
    "\n",
    "We also set the VGG model that we download to be not trainable. We don't want to overwrite all of the training that already exists, coming from the original training. What we want to be trained are the final dense parts we added on to classify our specific scenario. All the weights in the convolutional layers are kept the same, as they have been developed through large amounts of training; the weights in the fully connected layers will be trained, resulting in a model that combines the \"sight\" of the pretrained model with the context of what we are trying to classify. The VGG bits will just show as though they are one layer in our model, and for training purposes that makes sense. We can also see in the \"trainable params\" listing in the summary, the large number of weights in that VGG section we are borrowing are not trainable - that's the smart part of the model. \n",
    "\n",
    "<b>Note:</b> I think the \"top\" label is a bit misleading, as it isn't really the top, it is the part at the end that shows at the bottom of a summary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作模型\n",
    "\n",
    "我們採用沒有頂層的模型，設置輸入圖像大小，然後添加我們自己的分類器。 加載模型很簡單，只需指定幾件事：\n",
    "<ul>\n",
    "<li> weights=\"imagenet\" - 告訴模型使用來自其 imagenet 訓練的權重。 這就是帶來“智慧”的原因，所以我們想要它。\n",
    "<li> include_top=False - 告訴模型不要引入我們想要替換的分類器位。\n",
    "<li> input_shape - 模型在特定數據大小 (224x224x3) 上進行訓練。 我們可以通過更改輸入大小來重新調整它的用途。\n",
    "</ul>\n",
    "\n",
    "我們還將下載的 VGG 模型設置為不可訓練。 我們不想覆蓋所有已經存在的來自原始訓練的訓練。 我們想要訓練的是我們添加的最終密集部分，用於對我們的特定場景進行分類。 \n",
    "\n",
    "卷積層中的所有權重保持不變，因為它們是通過大量訓練形成的； 全連接層中的權重將被訓練，從而產生一個模型，該模型將預訓練模型的“視覺”與我們試圖分類的上下文相結合。 \n",
    "\n",
    "VGG 位將只顯示為好像它們是我們模型中的一層，並且出於訓練目的，這是有意義的。 \n",
    "\n",
    "我們還可以在摘要中的“可訓練參數”列表中看到，我們藉用的 VGG 部分中的大量權重不可訓練——這是模型的智能部分。\n",
    "\n",
    "<b>注意：</b> 我認為“頂部”標籤有點誤導，因為它並不是真正的頂部，它是最後顯示在摘要底部的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,692,869\n",
      "Trainable params: 12,978,181\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, img_depth))\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "# Add Dense Stuff\n",
    "flatten_layer = Flatten()\n",
    "dense_layer_1 = Dense(512, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "drop_layer_1 = Dropout(.2)\n",
    "dense_layer_2 = Dense(256, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "prediction_layer = Dense(5)\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    drop_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and Train\n",
    "\n",
    "Once the new Frakenstein model is built we finish the training process as we normally would. The only difference is that here the weights of the VGG part of the model are not being adjusted during the backpropagation steps, only the weights in the layers that we added at the end are. For many, if not most, applications, this approach of adapting a pretrained model will give the best real world results. Unless you happen to live in a data centre, you probably lack both the data and the processing capacity to train any model from scratch to be as good as those that we can download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "92/92 [==============================] - 606s 7s/step - loss: 15.4455 - accuracy: 0.7302 - val_loss: 11.4056 - val_accuracy: 0.8052\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 713s 8s/step - loss: 10.1482 - accuracy: 0.8716 - val_loss: 9.9185 - val_accuracy: 0.8270\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 822s 9s/step - loss: 8.2970 - accuracy: 0.9200 - val_loss: 8.3452 - val_accuracy: 0.8583\n",
      "Epoch 4/10\n",
      "74/92 [=======================>......] - ETA: 2:01 - loss: 7.0612 - accuracy: 0.9358"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m stopping_callback \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(filepath\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweights/VGG/initial/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mtime_stamp\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel.hdf5\u001b[39m\u001b[39m\"\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_ds,\n\u001b[0;32m     16\u001b[0m             epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     17\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     18\u001b[0m             validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[0;32m     19\u001b[0m             callbacks\u001b[39m=\u001b[39;49m[tensorboard_callback, stopping_callback, checkpoint_callback])\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Elsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),  \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/VGG/initial/\" + time_stamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "stopping_callback = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True, mode=\"max\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/VGG/initial/\"+time_stamp+\"model.hdf5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "model.fit(train_ds,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=[tensorboard_callback, stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune Models\n",
    "\n",
    "Lastly, we can adapt the entire model to our data. We'll unfreeze the original model, and then train the model again. The key addition here is that we set the learning rate to be extremely low (here it is 2 orders of magnitude smaller than the default) so the model doesn't totally rewrite all of the weights while training, rather it will only change a little bit - fine tuning its predictions to the actual data! Here the oringal convolutional layers are trainable, and the weights will be adjusted during training, but we dial the learning rate way down so that our changes only impact the model a little bit. This is a greater degree of fine tuning than we get when we lock the VGG layers, but it is still mainly relying on the previous training of the VGG model.\n",
    "\n",
    "The end result is a model that can take advantage of all of the training that the original model received before we downloaded it. That ability of extracting features from images is then reapplied to our data for making predictions based on the features identified in the original model. Finally we take the entire model and just gently train it to be a little more suited to our data. The best of all worlds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a copy of the above model for next test. \n",
    "copy_model = model\n",
    "\n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    ")\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/VGG/fine_tune/\" + time_stamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/VGG/fine_tune/\"+time_stamp+\"model.hdf5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=1, callbacks=[tensorboard_callback, stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer + Fine Tuning Results\n",
    "\n",
    "Yay, that's probably pretty accurate! In initial testing with 1 epoch, I got results around 80% before the fine tuning, and over 85% after the fine tuning. That's with 1 epoch! Other runs where we allow it to tune more trend to be even better - allowing 5 epochs of training + 5 epochs of fine tuning, my validation accuracy was around 90% and the training accuracy was nearing 100% - we could likely do even better with more aggressive regularization. \n",
    "\n",
    "This will likely be a great approach for something like image recognition!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the Model Looking?\n",
    "\n",
    "One of the things that we may wonder is how our models make decisions, or what are the looking at to do so. A tool that can help illustrate that is called a salience map. A salience map shows a visual representation of what parts of the image are impacting the final prediction the most. While the CNN process is largely a black box, this is one way to gain a little insight on what is going on. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Focus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from tensorflow.keras.preprocessing.image import load_img\n",
    "    from tf_keras_vis.utils.scores import CategoricalScore\n",
    "    \n",
    "    # Image titles\n",
    "    image_titles = ['daisy', 'roses', 'tulips']\n",
    "    score = CategoricalScore([0, 2, 4])\n",
    "\n",
    "    # Load images and Convert them to a Numpy array\n",
    "    img1 = load_img('/root/.keras/datasets/flower_photos/daisy/100080576_f52e8ee070_n.jpg', target_size=(224, 224))\n",
    "    img2 = load_img('/root/.keras/datasets/flower_photos/roses/10894627425_ec76bbc757_n.jpg', target_size=(224, 224))\n",
    "    #img3 = load_img('/root/.keras/datasets/flower_photos/tulips/10128546863_8de70c610d.jpg', target_size=(224, 224))\n",
    "      img3 = load_img(\"/root/.keras/datasets/flower_photos/tulips/12764617214_12211c6a0c_m.jpg\", target_size=(224, 224))\n",
    "    images = np.asarray([np.array(img1), np.array(img2), np.array(img3)])\n",
    "\n",
    "    # Preparing input data for VGG16\n",
    "    X = preprocess_input(images)\n",
    "\n",
    "    # Rendering\n",
    "    f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "    for i, title in enumerate(image_titles):\n",
    "        ax[i].set_title(title, fontsize=16)\n",
    "        ax[i].imshow(images[i])\n",
    "        ax[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "#### Show Saliency\n",
    "\n",
    "The bright spots in the image are the areas that the model is focusing on to make its prediction. The darker areas are not as important. We can think of this as a rough approximation of feature importance from something like a tree, only in 2D. \n",
    "\n",
    "Using a salience map in detail to tune our models goes beyond the scope of what we are going to do, but it does allow us to get at least some insight. The most direct thing that we can do is that we can figure out which parts of images are relied on for the model to do its job, this can help us to understand what the model is looking for. For something like image recognition, this could lead you to think about how the images are processed - for example, it is very common to snip out parts of larger images, usually the center, for use in a predictive model. This could show us evidence of if we are capturing the important parts or if we should modify that image prep process. I we were considering the padding decision, this could also give us an idea of if the edges matter or not for the model's predicitons. If the most important parts of the image are the \"thing\", then we are likely doing well, if they most important parts are background or the periphery, then we may been to change some things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "    replace2linear = ReplaceToLinear()\n",
    "    from tensorflow.keras import backend as K\n",
    "    from tf_keras_vis.saliency import Saliency\n",
    "    # from tf_keras_vis.utils import normalize\n",
    "\n",
    "    # Create Saliency object.\n",
    "    saliency = Saliency(model, model_modifier=replace2linear, clone=True)\n",
    "\n",
    "    # Generate saliency map\n",
    "    saliency_map = saliency(score, X)\n",
    "    # Generate saliency map with smoothing that reduce noise by adding noise\n",
    "    saliency_map = saliency(score,\n",
    "                            X,\n",
    "                            smooth_samples=20, # The number of calculating gradients iterations.\n",
    "                            smooth_noise=0.20) # noise spread level.\n",
    "\n",
    "    # Render\n",
    "    f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "    for i, title in enumerate(image_titles):\n",
    "        ax[i].set_title(title, fontsize=14)\n",
    "        ax[i].imshow(saliency_map[i], cmap='jet')\n",
    "        ax[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Drastic Retraining\n",
    "\n",
    "If we are extra ambitious we can also potentially slice the model even deeper, and take smaller portions to mix with our own models. The farther \"into\" the model you slice, the more of the original training will be removed and the more the model will learn from our training data. If done, this is a balancing act - we want to keep all of the smarts that the model has gotten from the original training, while getting the benefits of adaptation to our data. \n",
    "\n",
    "This is something that is hard to just eyeball - to splice parts of models together and create something that is actually superior likely requries a lot of experimentation, a solid understanding of the model's problem you're addressing, and some domain knowledge. For something like this adaptation of the VGG model, we'd probably start with some idea of what the model was weak at, build an understanding of what types of features it was extracting along the way, and insert our own layers where we think it would be most beneficial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, img_depth))\n",
    "#base_model.trainable = False ## Not trainable weights\n",
    "base_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freeze the First 12 Layers\n",
    "\n",
    "We will set the first 12 layers to be frozen, and leave the rest open to be trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:12]:\n",
    "    layer.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Retraining\n",
    "\n",
    "Now we have larger portions of the model that can be trained. We will be losing some of the pretrained knowldge, replacing it with the training coming from our data. If we look at the trainable params above, there are a bunch that are trainable and a bunch that aren't.\n",
    "\n",
    "We are playing with fire here! Taking away more and more of the \"smart\" model will be risky for actual performance, we are pretty likely to make things worse as we go father and farther into removing the old training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Dense Stuff\n",
    "flatten_layer = Flatten()\n",
    "dense_layer_1 = Dense(512, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "dense_layer_2 = Dense(256, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "prediction_layer = Dense(5)\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    ")\n",
    "            \n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/VGG/drastic/\" + time_stamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/VGG/drastic/\"+time_stamp+\"model.hdf5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "model.fit(train_ds,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=[tensorboard_callback, stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We likely see worse results when retraining more of the model, that's to be expected. In general, replacing the classifier and possibly some low learning rate fine tuning is the best solution for most cases like this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - ResNet50\n",
    "\n",
    "This is another pretrained network, containing 50 layers. We can use this one similarly to the last. Try to use transfer learning along with some of your added layers to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def preprocess50(images, labels):\n",
    "  return tf.keras.applications.resnet50.preprocess_input(images), labels\n",
    "\n",
    "train_ds = train_ds_orig.map(preprocess50)\n",
    "val_ds = val_ds_orig.map(preprocess50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, img_depth))\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "# Add Dense Stuff\n",
    "flatten_layer = Flatten()\n",
    "dense_layer_1 = Dense(512, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "dense_layer_2 = Dense(256, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "dense_layer_3 = Dense(96, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "prediction_layer = Dense(5)\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    dense_layer_3,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train New Classifier\n",
    "\n",
    "Train model with new classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "            optimizer=\"adam\", \n",
    "            metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"))\n",
    "            \n",
    "log_dir = \"logs/50/initial/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/50/initial/\"+time_stamp+\"model.hdf5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "model.fit(train_ds,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=[tensorboard_callback, stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt Retraining Entire Model to Fine Tune\n",
    "\n",
    "We can attempt to unlock the model and retrain in fine tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-6),  # Low learning rate\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    ")\n",
    "\n",
    "log_dir = \"logs/50/fine_tune/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/50/fine_tune/\"+time_stamp+\"model.hdf5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=1, callbacks=[tensorboard_callback, stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning Conclusion\n",
    "\n",
    "Transfer learning is common, especially when working with things like images. Pretrained models that have seen millions upon millions of images get very good at \"understanding\" what is in an image, or extracting important features from those images. This basic ability to \"see\" image data is interchangeable between different types of image tasks that we may want to do. For image data, natural language, audio, video, it is likely that one of these large models will be more capable of extracting features from the data than we could ever hope to do from scratch. Since the basics of \"seeing a thing\" or \"reading a sentence\" is the same no matter the specific application, that ability to process the data that our pretrained models have can be repurposed to our specific ends. \n",
    "\n",
    "We can see lots of scenarios in the real world where people are adapting image recognition models trained by Google to do things like recognize objects in their home security system, or language models like the GPT family being adapted to better understand domain specific language. We'll likely see more of this, as the benefits of training on massive amounts of data are hard, if not impossible, to replicate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c62bd1151ad2554f1dd664720f74a12e4919cab8bb2c6e38e1699537fee996f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
